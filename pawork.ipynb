{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import argparse\nimport math\nimport time\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport logging\nfrom datetime import datetime\nimport os\nfrom torch.utils.data import Dataset, DataLoader\nfrom os.path import join, exists\nfrom torch.nn import CrossEntropyLoss\nfrom tqdm import tqdm\nfrom torch.nn import DataParallel\nimport transformers\nimport pickle\nimport sys\nfrom sklearn.model_selection import train_test_split\nfrom transformers import GPT2TokenizerFast, GPT2LMHeadModel, GPT2Config\nfrom transformers import BertTokenizerFast\nimport pandas as pd\nimport torch.nn.utils.rnn as rnn_utils\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-12-20T03:02:38.814421Z","iopub.execute_input":"2022-12-20T03:02:38.816183Z","iopub.status.idle":"2022-12-20T03:02:43.754101Z","shell.execute_reply.started":"2022-12-20T03:02:38.816009Z","shell.execute_reply":"2022-12-20T03:02:43.752185Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## 全部配置","metadata":{}},{"cell_type":"code","source":"class MyConfig():\n    def __init__(self):\n        self.device = '3'    #设置使用哪些显卡\n        self.no_cuda = False    #不使用GPU进行训练\n        self.vocab_path = '/kaggle/input/paworks/vocab.txt'    #词表路径\n        self.model_config = '/kaggle/input/paworks/config.json'    #设置模型参数\n        self.train_path = '/kaggle/input/paworks/train1.pkl'    #训练集路径\n        self.max_len = 150    #训练时，输入数据的最大长度\n        self.log_path = 'train.log'    #训练日志存放位置\n        self.log = True    #是否记录日志\n        self.ignore_index = -100    #对于ignore_index的label token不计算梯度\n        self.epochs = 100    #训练的最大轮次\n        self.batch_size = 8    #训练的batch size\n        self.gpu0_bsz = 10    #0号卡的batch size\n        self.lr = 2.6e-5    #学习率\n        self.eps = 1.0e-09    #衰减率\n        self.log_step = 1    #多少步汇报一次loss\n        self.gradient_accumulation_steps = 4    #梯度积累\n        self.max_grad_norm = 2.0    #参数的梯度范数\n        self.save_model_path = 'model'    #模型输出路径\n        self.pretrained_model = '/kaggle/input/paworks/pytorch_model.bin'    #预训练的模型的路径\n        self.num_workers = 8    #dataloader加载数据时使用的线程数量\n        self.patience = 0    #用于early stopping,设为0时,不进行early stopping.early stop得到的模型的生成效果不一定会更好。\n        self.warmup_steps = 4000    #warm up步数\n        self.val_num = 8    #验证集大小","metadata":{"execution":{"iopub.status.busy":"2022-12-20T03:02:45.276469Z","iopub.execute_input":"2022-12-20T03:02:45.277573Z","iopub.status.idle":"2022-12-20T03:02:45.287169Z","shell.execute_reply.started":"2022-12-20T03:02:45.277524Z","shell.execute_reply":"2022-12-20T03:02:45.285745Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# 代码主体","metadata":{}},{"cell_type":"code","source":"class MyDataset(Dataset):\n    \"\"\"\n\n    \"\"\"\n\n    def __init__(self, input_list, max_len):\n        self.input_list = input_list\n        self.max_len = max_len\n\n    def __getitem__(self, index):\n        input_ids = self.input_list[index]\n        input_ids = input_ids[:self.max_len]\n        input_ids = torch.tensor(input_ids, dtype=torch.long)\n        return input_ids\n\n    def __len__(self):\n        return len(self.input_list)\n    \n\n\ndef run():\n    my_config = MyConfig()\n\n    # 设置使用哪些显卡进行训练\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = my_config.device\n\n    my_config.cuda = not my_config.no_cuda\n\n    if my_config.batch_size < 2048 and my_config.warmup_steps <= 4000:\n        print('[Warning] The warmup steps may be not enough.\\n' \\\n              '(sz_b, warmup) = (2048, 4000) is the official setting.\\n' \\\n              'Using smaller batch w/o longer warmup may cause ' \\\n              'the warmup stage ends with only little data trained.')\n\n    # 创建日志对象\n    logger = create_logger(my_config)\n    # 当用户使用GPU,并且GPU可用时\n    my_config.cuda = torch.cuda.is_available() and not my_config.no_cuda\n    device = 'cuda:0' if my_config.cuda else 'cpu'\n    my_config.device = device\n    logger.info('using device:{}'.format(device))\n\n    # 初始化tokenizer\n    tokenizer = BertTokenizerFast(vocab_file=my_config.vocab_path, sep_token=\"[SEP]\", pad_token=\"[PAD]\", cls_token=\"[CLS]\")\n    my_config.sep_id = tokenizer.sep_token_id\n    my_config.pad_id = tokenizer.pad_token_id\n    my_config.cls_id = tokenizer.cls_token_id\n\n    # 创建模型的输出目录\n    if not os.path.exists(my_config.save_model_path):\n        os.mkdir(my_config.save_model_path)\n\n    # 创建模型\n    if my_config.pretrained_model:  # 加载预训练模型\n        print(f'model_config_path:{my_config.model_config}  model:{my_config.pretrained_model}')\n        model_config = GPT2Config.from_json_file(my_config.model_config)\n        model = GPT2LMHeadModel.from_pretrained(my_config.pretrained_model, config=model_config)\n    else:  # 初始化模型\n        model_config = GPT2Config.from_json_file(my_config.model_config)\n        model = GPT2LMHeadModel(config=model_config)\n    model = model.to(device)\n    logger.info('model config:\\n{}'.format(model.config.to_json_string()))\n    assert model.config.vocab_size == tokenizer.vocab_size\n\n    # 并行训练模型\n    if my_config.cuda and torch.cuda.device_count() > 1:\n        model = DataParallel(model).cuda()\n        # model = BalancedDataParallel(my_config.gpu0_bsz, model, dim=0).cuda()\n        logger.info(\"use GPU {} to train\".format(my_config.device))\n\n    # 计算模型参数数量\n    num_parameters = 0\n    parameters = model.parameters()\n    for parameter in parameters:\n        num_parameters += parameter.numel()\n    logger.info('number of model parameters: {}'.format(num_parameters))\n\n    # 记录参数设置\n    logger.info(\"my_config:{}\".format(my_config))\n\n    # 加载训练集和验证集\n    # ========= Loading Dataset ========= #\n    train_dataset, validate_dataset = load_dataset(logger, my_config)\n\n    train(model, logger, train_dataset, validate_dataset, my_config)\n    \ndef create_logger(my_config):\n    \"\"\"\n    将日志输出到日志文件和控制台\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    logger.setLevel(logging.INFO)\n\n    formatter = logging.Formatter(\n        '%(asctime)s - %(levelname)s - %(message)s')\n\n    # 创建一个handler，用于写入日志文件\n    file_handler = logging.FileHandler(\n        filename=my_config.log_path)\n    file_handler.setFormatter(formatter)\n    file_handler.setLevel(logging.INFO)\n    logger.addHandler(file_handler)\n\n    # 创建一个handler，用于将日志输出到控制台\n    console = logging.StreamHandler()\n    console.setLevel(logging.DEBUG)\n    console.setFormatter(formatter)\n    logger.addHandler(console)\n\n    return logger    \n\ndef load_dataset(logger, my_config):\n    \"\"\"\n    加载训练集和验证集\n    \"\"\"\n    logger.info(\"loading training dataset and validating dataset\")\n    train_path = my_config.train_path\n\n    with open(train_path, \"rb\") as f:\n        input_list = pickle.load(f)\n\n    # 划分训练集与验证集\n    val_num = my_config.val_num\n    input_list_train = input_list[val_num:]\n    input_list_val = input_list[:val_num]\n    # test\n    # input_list_train = input_list_train[:24]\n    # input_list_val = input_list_val[:24]\n    print(f'train {len(input_list_train)} {input_list_train}')\n    print(f'valid {len(input_list_val)} {input_list_val}')\n    train_dataset = MyDataset(input_list_train, my_config.max_len)\n    val_dataset = MyDataset(input_list_val, my_config.max_len)\n\n\n    return train_dataset, val_dataset\n\ndef train_epoch(model, train_dataloader, optimizer, scheduler, logger,\n                epoch, my_config):\n    model.train()\n    device = my_config.device\n    # pad_id = my_config.pad_id\n    # sep_id = my_config.sep_id\n    ignore_index = my_config.ignore_index\n    epoch_start_time = datetime.now()\n    total_loss = 0  # 记录下整个epoch的loss的总和\n\n    # epoch_correct_num:每个epoch中,output预测正确的word的数量\n    # epoch_total_num: 每个epoch中,output预测的word的总数量\n    epoch_correct_num, epoch_total_num = 0, 0\n\n    for batch_idx, (input_ids, labels) in enumerate(train_dataloader):\n        # 捕获cuda out of memory exception\n        try:\n            input_ids = input_ids.to(device)\n            labels = labels.to(device)\n            outputs = model.forward(input_ids, labels=labels)\n            logits = outputs.logits\n            loss = outputs.loss\n            loss = loss.mean()\n\n            # 统计该batch的预测token的正确数与总数\n            batch_correct_num, batch_total_num = calculate_acc(logits, labels, ignore_index=ignore_index)\n            # 统计该epoch的预测token的正确数与总数\n            epoch_correct_num += batch_correct_num\n            epoch_total_num += batch_total_num\n            # 计算该batch的accuracy\n            batch_acc = batch_correct_num / batch_total_num\n\n            total_loss += loss.item()\n            if my_config.gradient_accumulation_steps > 1:\n                loss = loss / my_config.gradient_accumulation_steps\n\n            loss.backward()\n            # 梯度裁剪\n            torch.nn.utils.clip_grad_norm_(model.parameters(), my_config.max_grad_norm)\n\n            # 进行一定step的梯度累计之后，更新参数\n            if (batch_idx + 1) % my_config.gradient_accumulation_steps == 0:\n                # 更新参数\n                optimizer.step()\n                # 更新学习率\n                scheduler.step()\n                # 清空梯度信息\n                optimizer.zero_grad()\n\n            if (batch_idx + 1) % my_config.log_step == 0:\n                logger.info(\n                    \"batch {} of epoch {}, loss {}, batch_acc {}, lr {}\".format(\n                        batch_idx + 1, epoch + 1, loss.item() * my_config.gradient_accumulation_steps, batch_acc,\n                        scheduler.get_lr()))\n\n            del input_ids, outputs\n\n        except RuntimeError as exception:\n            if \"out of memory\" in str(exception):\n                logger.info(\"WARNING: ran out of memory\")\n                if hasattr(torch.cuda, 'empty_cache'):\n                    torch.cuda.empty_cache()\n            else:\n                logger.info(str(exception))\n                raise exception\n\n    # 记录当前epoch的平均loss与accuracy\n    epoch_mean_loss = total_loss / len(train_dataloader)\n    epoch_mean_acc = epoch_correct_num / epoch_total_num\n    logger.info(\n        \"epoch {}: loss {}, predict_acc {}\".format(epoch + 1, epoch_mean_loss, epoch_mean_acc))\n\n    # save model\n    logger.info('saving model for epoch {}'.format(epoch + 1))\n    model_path = join(my_config.save_model_path, 'epoch{}'.format(epoch + 1))\n    if not os.path.exists(model_path):\n        os.mkdir(model_path)\n    model_to_save = model.module if hasattr(model, 'module') else model\n    model_to_save.save_pretrained(model_path)\n    logger.info('epoch {} finished'.format(epoch + 1))\n    epoch_finish_time = datetime.now()\n    logger.info('time for one epoch: {}'.format(epoch_finish_time - epoch_start_time))\n\n    return epoch_mean_loss\n\n\ndef validate_epoch(model, validate_dataloader, logger, epoch, my_config):\n    logger.info(f\"start validating {len(validate_dataloader)}\")\n    model.eval()\n    device = my_config.device\n    # pad_id = my_config.pad_id\n    # sep_id = my_config.sep_id\n    ignore_index = my_config.ignore_index\n    epoch_start_time = datetime.now()\n    total_loss = 0\n    # 捕获cuda out of memory exception\n    try:\n        with torch.no_grad():\n            for batch_idx, (input_ids, labels) in enumerate(validate_dataloader):\n                input_ids = input_ids.to(device)\n                labels = labels.to(device)\n                outputs = model.forward(input_ids, labels=labels)\n                logits = outputs.logits\n                loss = outputs.loss\n                loss = loss.mean()\n\n                total_loss += loss.item()\n                del input_ids, outputs\n\n            # 记录当前epoch的平均loss\n            epoch_mean_loss = total_loss / len(validate_dataloader)\n            logger.info(\n                \"validate epoch {}: loss {}\".format(epoch + 1, epoch_mean_loss))\n            epoch_finish_time = datetime.now()\n            logger.info('time for validating one epoch: {}'.format(epoch_finish_time - epoch_start_time))\n            return epoch_mean_loss\n    except RuntimeError as exception:\n        if \"out of memory\" in str(exception):\n            logger.info(\"WARNING: ran out of memory\")\n            if hasattr(torch.cuda, 'empty_cache'):\n                torch.cuda.empty_cache()\n        else:\n            logger.info(str(exception))\n            raise exception\n\n\ndef train(model, logger, train_dataset, validate_dataset, my_config):\n    train_dataloader = DataLoader(\n        train_dataset, batch_size=my_config.batch_size, shuffle=True, num_workers=my_config.num_workers, collate_fn=collate_fn,\n        drop_last=True\n    )\n    validate_dataloader = DataLoader(validate_dataset, batch_size=my_config.batch_size, shuffle=True,\n                                     num_workers=my_config.num_workers, collate_fn=collate_fn, drop_last=True)\n#     early_stopping = EarlyStopping(my_config.patience, verbose=True, save_path=my_config.save_model_path)\n    t_total = len(train_dataloader) // my_config.gradient_accumulation_steps * my_config.epochs\n    optimizer = transformers.AdamW(model.parameters(), lr=my_config.lr, eps=my_config.eps)\n    # scheduler = transformers.WarmupLinearSchedule(optimizer, warmup_steps=my_config.warmup_steps, t_total=t_total)\n    scheduler = transformers.get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=my_config.warmup_steps, num_training_steps=t_total\n    )\n\n    logger.info('starting training')\n\n    # 用于记录每个epoch训练和验证的loss\n    train_losses, validate_losses = [], []\n    # 记录验证集的最小loss\n    best_val_loss = 10000\n    # 开始训练\n    for epoch in range(my_config.epochs):\n        # ========== train ========== #\n        train_loss = train_epoch(\n            model=model, train_dataloader=train_dataloader,\n            optimizer=optimizer, scheduler=scheduler,\n            logger=logger, epoch=epoch, my_config=my_config)\n        train_losses.append(train_loss)\n\n        # ========== validate ========== #\n        validate_loss = validate_epoch(\n            model=model, validate_dataloader=validate_dataloader,\n            logger=logger, epoch=epoch, my_config=my_config)\n        validate_losses.append(validate_loss)\n\n        # 保存当前困惑度最低的模型，困惑度低，模型的生成效果不一定会越好\n        if validate_loss < best_val_loss:\n            best_val_loss = validate_loss\n            logger.info('saving current best model for epoch {}'.format(epoch + 1))\n            model_path = join(my_config.save_model_path, 'min_ppl_model'.format(epoch + 1))\n            if not os.path.exists(model_path):\n                os.mkdir(model_path)\n            model_to_save = model.module if hasattr(model, 'module') else model\n            model_to_save.save_pretrained(model_path)\n\n        #  如果patience=0,则不进行early stopping\n        if my_config.patience == 0:\n            continue\n        # 这里不知道为何无法导入EarlyStopping\n#         early_stopping(validate_loss, model)\n#         if early_stopping.early_stop:\n#             logger.info(\"Early stopping\")\n#             break\n    logger.info('training finished')\n    logger.info(\"train_losses:{}\".format(train_losses))\n    logger.info(\"validate_losses:{}\".format(validate_losses))\n\n\ndef caculate_loss(logit, target, pad_idx, smoothing=True):\n    if smoothing:\n        logit = logit[..., :-1, :].contiguous().view(-1, logit.size(2))\n        target = target[..., 1:].contiguous().view(-1)\n\n        eps = 0.1\n        n_class = logit.size(-1)\n\n        one_hot = torch.zeros_like(logit).scatter(1, target.view(-1, 1), 1)\n        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n        log_prb = F.log_softmax(logit, dim=1)\n\n        non_pad_mask = target.ne(pad_idx)\n        loss = -(one_hot * log_prb).sum(dim=1)\n        loss = loss.masked_select(non_pad_mask).mean()  # average later\n    else:\n        # loss = F.cross_entropy(predict_logit, target, ignore_index=pad_idx)\n        logit = logit[..., :-1, :].contiguous().view(-1, logit.size(-1))\n        labels = target[..., 1:].contiguous().view(-1)\n        loss = F.cross_entropy(logit, labels, ignore_index=pad_idx)\n    return loss\n\n\ndef calculate_acc(logit, labels, ignore_index=-100):\n    logit = logit[..., :-1, :].contiguous().view(-1, logit.size(-1))\n    labels = labels[..., 1:].contiguous().view(-1)\n\n    _, logit = logit.max(dim=-1)  # 对于每条数据，返回最大的index\n    # 进行非运算，返回一个tensor，若labels的第i个位置为pad_id，则置为0，否则为1\n    non_pad_mask = labels.ne(ignore_index)\n    n_correct = logit.eq(labels).masked_select(non_pad_mask).sum().item()\n    n_word = non_pad_mask.sum().item()\n    return n_correct, n_word\n\ndef collate_fn(batch):\n    input_ids = rnn_utils.pad_sequence(batch, batch_first=True, padding_value=0)\n    labels = rnn_utils.pad_sequence(batch, batch_first=True, padding_value=-100)\n    return input_ids, labels\nrun()","metadata":{"execution":{"iopub.status.busy":"2022-12-20T03:05:31.128056Z","iopub.execute_input":"2022-12-20T03:05:31.128548Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"2022-12-20 03:05:31,189 - INFO - using device:cpu\n2022-12-20 03:05:31,189 - INFO - using device:cpu\n2022-12-20 03:05:31,189 - INFO - using device:cpu\n2022-12-20 03:05:31,189 - INFO - using device:cpu\n2022-12-20 03:05:31,189 - INFO - using device:cpu\n2022-12-20 03:05:31,189 - INFO - using device:cpu\n","output_type":"stream"},{"name":"stdout","text":"[Warning] The warmup steps may be not enough.\n(sz_b, warmup) = (2048, 4000) is the official setting.\nUsing smaller batch w/o longer warmup may cause the warmup stage ends with only little data trained.\nmodel_config_path:/kaggle/input/paworks/config.json  model:/kaggle/input/paworks/pytorch_model.bin\n","output_type":"stream"},{"name":"stderr","text":"2022-12-20 03:05:32,285 - INFO - model config:\n{\n  \"_name_or_path\": \"/kaggle/input/paworks/pytorch_model.bin\",\n  \"activation_function\": \"gelu_new\",\n  \"attn_pdrop\": 0.1,\n  \"bos_token_id\": 50256,\n  \"embd_pdrop\": 0.1,\n  \"eos_token_id\": 50256,\n  \"id2label\": {\n    \"0\": \"LABEL_0\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0\n  },\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"gpt2\",\n  \"n_ctx\": 300,\n  \"n_embd\": 768,\n  \"n_head\": 12,\n  \"n_inner\": null,\n  \"n_layer\": 10,\n  \"n_positions\": 300,\n  \"output_past\": true,\n  \"reorder_and_upcast_attn\": false,\n  \"resid_pdrop\": 0.1,\n  \"scale_attn_by_inverse_layer_idx\": false,\n  \"scale_attn_weights\": true,\n  \"summary_activation\": null,\n  \"summary_first_dropout\": 0.1,\n  \"summary_proj_to_labels\": true,\n  \"summary_type\": \"cls_index\",\n  \"summary_use_proj\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 13317\n}\n\n2022-12-20 03:05:32,285 - INFO - model config:\n{\n  \"_name_or_path\": \"/kaggle/input/paworks/pytorch_model.bin\",\n  \"activation_function\": \"gelu_new\",\n  \"attn_pdrop\": 0.1,\n  \"bos_token_id\": 50256,\n  \"embd_pdrop\": 0.1,\n  \"eos_token_id\": 50256,\n  \"id2label\": {\n    \"0\": \"LABEL_0\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0\n  },\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"gpt2\",\n  \"n_ctx\": 300,\n  \"n_embd\": 768,\n  \"n_head\": 12,\n  \"n_inner\": null,\n  \"n_layer\": 10,\n  \"n_positions\": 300,\n  \"output_past\": true,\n  \"reorder_and_upcast_attn\": false,\n  \"resid_pdrop\": 0.1,\n  \"scale_attn_by_inverse_layer_idx\": false,\n  \"scale_attn_weights\": true,\n  \"summary_activation\": null,\n  \"summary_first_dropout\": 0.1,\n  \"summary_proj_to_labels\": true,\n  \"summary_type\": \"cls_index\",\n  \"summary_use_proj\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 13317\n}\n\n2022-12-20 03:05:32,285 - INFO - model config:\n{\n  \"_name_or_path\": \"/kaggle/input/paworks/pytorch_model.bin\",\n  \"activation_function\": \"gelu_new\",\n  \"attn_pdrop\": 0.1,\n  \"bos_token_id\": 50256,\n  \"embd_pdrop\": 0.1,\n  \"eos_token_id\": 50256,\n  \"id2label\": {\n    \"0\": \"LABEL_0\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0\n  },\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"gpt2\",\n  \"n_ctx\": 300,\n  \"n_embd\": 768,\n  \"n_head\": 12,\n  \"n_inner\": null,\n  \"n_layer\": 10,\n  \"n_positions\": 300,\n  \"output_past\": true,\n  \"reorder_and_upcast_attn\": false,\n  \"resid_pdrop\": 0.1,\n  \"scale_attn_by_inverse_layer_idx\": false,\n  \"scale_attn_weights\": true,\n  \"summary_activation\": null,\n  \"summary_first_dropout\": 0.1,\n  \"summary_proj_to_labels\": true,\n  \"summary_type\": \"cls_index\",\n  \"summary_use_proj\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 13317\n}\n\n2022-12-20 03:05:32,285 - INFO - model config:\n{\n  \"_name_or_path\": \"/kaggle/input/paworks/pytorch_model.bin\",\n  \"activation_function\": \"gelu_new\",\n  \"attn_pdrop\": 0.1,\n  \"bos_token_id\": 50256,\n  \"embd_pdrop\": 0.1,\n  \"eos_token_id\": 50256,\n  \"id2label\": {\n    \"0\": \"LABEL_0\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0\n  },\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"gpt2\",\n  \"n_ctx\": 300,\n  \"n_embd\": 768,\n  \"n_head\": 12,\n  \"n_inner\": null,\n  \"n_layer\": 10,\n  \"n_positions\": 300,\n  \"output_past\": true,\n  \"reorder_and_upcast_attn\": false,\n  \"resid_pdrop\": 0.1,\n  \"scale_attn_by_inverse_layer_idx\": false,\n  \"scale_attn_weights\": true,\n  \"summary_activation\": null,\n  \"summary_first_dropout\": 0.1,\n  \"summary_proj_to_labels\": true,\n  \"summary_type\": \"cls_index\",\n  \"summary_use_proj\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 13317\n}\n\n2022-12-20 03:05:32,285 - INFO - model config:\n{\n  \"_name_or_path\": \"/kaggle/input/paworks/pytorch_model.bin\",\n  \"activation_function\": \"gelu_new\",\n  \"attn_pdrop\": 0.1,\n  \"bos_token_id\": 50256,\n  \"embd_pdrop\": 0.1,\n  \"eos_token_id\": 50256,\n  \"id2label\": {\n    \"0\": \"LABEL_0\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0\n  },\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"gpt2\",\n  \"n_ctx\": 300,\n  \"n_embd\": 768,\n  \"n_head\": 12,\n  \"n_inner\": null,\n  \"n_layer\": 10,\n  \"n_positions\": 300,\n  \"output_past\": true,\n  \"reorder_and_upcast_attn\": false,\n  \"resid_pdrop\": 0.1,\n  \"scale_attn_by_inverse_layer_idx\": false,\n  \"scale_attn_weights\": true,\n  \"summary_activation\": null,\n  \"summary_first_dropout\": 0.1,\n  \"summary_proj_to_labels\": true,\n  \"summary_type\": \"cls_index\",\n  \"summary_use_proj\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 13317\n}\n\n2022-12-20 03:05:32,285 - INFO - model config:\n{\n  \"_name_or_path\": \"/kaggle/input/paworks/pytorch_model.bin\",\n  \"activation_function\": \"gelu_new\",\n  \"attn_pdrop\": 0.1,\n  \"bos_token_id\": 50256,\n  \"embd_pdrop\": 0.1,\n  \"eos_token_id\": 50256,\n  \"id2label\": {\n    \"0\": \"LABEL_0\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"LABEL_0\": 0\n  },\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"gpt2\",\n  \"n_ctx\": 300,\n  \"n_embd\": 768,\n  \"n_head\": 12,\n  \"n_inner\": null,\n  \"n_layer\": 10,\n  \"n_positions\": 300,\n  \"output_past\": true,\n  \"reorder_and_upcast_attn\": false,\n  \"resid_pdrop\": 0.1,\n  \"scale_attn_by_inverse_layer_idx\": false,\n  \"scale_attn_weights\": true,\n  \"summary_activation\": null,\n  \"summary_first_dropout\": 0.1,\n  \"summary_proj_to_labels\": true,\n  \"summary_type\": \"cls_index\",\n  \"summary_use_proj\": true,\n  \"transformers_version\": \"4.20.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 13317\n}\n\n2022-12-20 03:05:32,293 - INFO - number of model parameters: 81338112\n2022-12-20 03:05:32,293 - INFO - number of model parameters: 81338112\n2022-12-20 03:05:32,293 - INFO - number of model parameters: 81338112\n2022-12-20 03:05:32,293 - INFO - number of model parameters: 81338112\n2022-12-20 03:05:32,293 - INFO - number of model parameters: 81338112\n2022-12-20 03:05:32,293 - INFO - number of model parameters: 81338112\n2022-12-20 03:05:32,303 - INFO - my_config:<__main__.MyConfig object at 0x7f37f188eb10>\n2022-12-20 03:05:32,303 - INFO - my_config:<__main__.MyConfig object at 0x7f37f188eb10>\n2022-12-20 03:05:32,303 - INFO - my_config:<__main__.MyConfig object at 0x7f37f188eb10>\n2022-12-20 03:05:32,303 - INFO - my_config:<__main__.MyConfig object at 0x7f37f188eb10>\n2022-12-20 03:05:32,303 - INFO - my_config:<__main__.MyConfig object at 0x7f37f188eb10>\n2022-12-20 03:05:32,303 - INFO - my_config:<__main__.MyConfig object at 0x7f37f188eb10>\n2022-12-20 03:05:32,310 - INFO - loading training dataset and validating dataset\n2022-12-20 03:05:32,310 - INFO - loading training dataset and validating dataset\n2022-12-20 03:05:32,310 - INFO - loading training dataset and validating dataset\n2022-12-20 03:05:32,310 - INFO - loading training dataset and validating dataset\n2022-12-20 03:05:32,310 - INFO - loading training dataset and validating dataset\n2022-12-20 03:05:32,310 - INFO - loading training dataset and validating dataset\n","output_type":"stream"},{"name":"stdout","text":"train 57 [[101, 784, 720, 3221, 924, 7372, 782, 8043, 102, 924, 7372, 782, 2218, 3221, 680, 2832, 924, 782, 6370, 4989, 924, 7372, 1394, 1398, 8024, 2400, 2902, 4212, 1394, 1398, 5276, 2137, 2824, 2857, 6608, 985, 2772, 5314, 802, 924, 7372, 7032, 6569, 818, 4638, 924, 7372, 1062, 1385, 511, 102], [101, 924, 7372, 782, 1377, 809, 3221, 6443, 8043, 102, 3791, 782, 8038, 898, 4212, 517, 924, 7372, 3791, 518, 6392, 4989, 4638, 924, 7372, 1062, 1385, 809, 1350, 3791, 2526, 510, 6121, 3124, 3791, 6226, 6226, 2137, 4638, 1071, 800, 924, 7372, 5299, 5302, 511, 102, 5632, 4197, 782, 8038, 2208, 3144, 1744, 2157, 1038, 6387, 809, 702, 782, 6716, 819, 5307, 5852, 924, 7372, 689, 1218, 8024, 1963, 5739, 1744, 1227, 1394, 4852, 511, 2769, 1744, 3257, 3198, 679, 1038, 6387, 511, 102], [101, 924, 7372, 782, 3300, 784, 720, 721, 1218, 8043, 102, 6370, 5276, 6432, 3209, 8038, 1403, 2832, 924, 782, 6237, 7025, 924, 7372, 1394, 1398, 3340, 3621, 511, 102, 7599, 7372, 2824, 2857, 510, 924, 7372, 7032, 5314, 802, 8038, 1762, 1394, 1398, 5276, 2137, 2658, 1105, 1355, 4495, 3198, 2902, 5276, 2137, 924, 7372, 7032, 7583, 5314, 802, 924, 7372, 7032, 511, 102, 2145, 2787, 928, 2622, 924, 2166, 8038, 2190, 924, 7372, 1394, 1398, 1079, 2159, 510, 2832, 924, 782, 1350, 6158, 924, 7372, 782, 4685, 1068, 928, 2622, 924, 2166, 511, 102], [101, 924, 7372, 782, 3300, 784, 720, 3326, 1164, 8043, 102, 2461, 3326, 8038, 2900, 1394, 1398, 4638, 671, 3175, 2496, 752, 782, 3123, 2461, 1071, 1762, 924, 7372, 1394, 1398, 704, 1377, 809, 712, 2476, 4638, 3326, 1164, 8024, 6858, 2382, 3221, 2900, 924, 7372, 782, 3123, 2461, 1394, 1398, 6237, 7370, 3326, 1469, 2834, 6796, 3326, 511, 102], [101, 784, 720, 3221, 2832, 924, 782, 8043, 102, 2832, 924, 782, 2218, 3221, 3118, 802, 924, 6589, 4638, 782, 511, 102], [101, 2832, 924, 782, 1377, 809, 3221, 784, 720, 782, 8043, 102, 5632, 4197, 782, 8038, 1072, 3300, 2130, 1059, 3696, 752, 3326, 1164, 5543, 1213, 1469, 3696, 752, 6121, 711, 5543, 1213, 8024, 2190, 924, 7372, 3403, 4638, 1072, 3300, 924, 7372, 1164, 4660, 511, 102, 3791, 782, 8038, 821, 689, 1296, 855, 511, 102], [101, 2832, 924, 782, 3300, 784, 720, 3326, 1164, 8043, 102, 1359, 3291, 510, 5303, 3632, 924, 7372, 1394, 1398, 8039, 2900, 2137, 510, 1359, 3291, 1358, 4660, 782, 8039, 7566, 1357, 6842, 924, 7032, 511, 102], [101, 2832, 924, 782, 3300, 784, 720, 721, 1218, 8043, 102, 5373, 5287, 924, 6589, 8038, 1403, 924, 7372, 1062, 1385, 2902, 3198, 5373, 5287, 924, 6589, 511, 102, 1963, 2141, 1440, 4761, 8038, 1963, 2141, 1440, 4761, 924, 7372, 1062, 1385, 2990, 1139, 4638, 6158, 924, 7372, 782, 4638, 4685, 1068, 7309, 7579, 8024, 1139, 7372, 1400, 2990, 897, 3332, 3160, 5023, 511, 102], [101, 784, 720, 3221, 6158, 924, 7372, 782, 8043, 102, 2900, 1358, 924, 7372, 1394, 1398, 924, 7397, 8024, 775, 3300, 924, 7372, 7032, 6435, 3724, 3326, 4638, 782, 511, 102], [101, 6158, 924, 7372, 782, 1377, 809, 3221, 784, 720, 782, 8043, 102, 2399, 7977, 8038, 7028, 4565, 7372, 510, 2195, 7372, 671, 5663, 1762, 8114, 172, 8284, 1453, 2259, 8024, 2692, 1912, 7372, 3683, 6772, 2160, 3793, 511, 102, 978, 2434, 8038, 1278, 4545, 7372, 1469, 7028, 4565, 7372, 3417, 924, 698, 3419, 8024, 7444, 6206, 978, 2434, 1440, 4761, 8024, 2692, 1912, 7372, 671, 5663, 3766, 3300, 511, 102, 3800, 8038, 517, 924, 7372, 3791, 518, 6226, 2137, 8038, 2832, 924, 782, 679, 2533, 711, 3187, 3696, 752, 6121, 711, 5543, 1213, 782, 2832, 924, 809, 3647, 767, 711, 5314, 802, 3340, 816, 4638, 782, 6716, 924, 7372, 511, 102], [101, 6158, 924, 7372, 782, 3300, 784, 720, 3326, 1164, 8043, 102, 1398, 2692, 1359, 3291, 1358, 4660, 782, 8038, 2832, 924, 782, 1762, 1359, 3291, 1358, 4660, 782, 3198, 8024, 7444, 5307, 2533, 6158, 924, 7372, 782, 1398, 2692, 511, 102, 4509, 6435, 924, 7372, 4415, 6608, 8038, 1394, 1398, 5276, 2137, 4638, 924, 7372, 752, 3125, 1355, 4495, 1400, 8024, 6158, 924, 7372, 782, 3300, 3326, 1403, 924, 7372, 782, 4509, 6435, 4415, 6608, 511, 102], [101, 784, 720, 3221, 924, 7397, 3309, 8043, 102, 924, 7397, 3309, 3221, 924, 7372, 1062, 1385, 2990, 897, 924, 7397, 4638, 2399, 7361, 8024, 738, 2218, 3221, 924, 7372, 1062, 1385, 833, 2190, 872, 6566, 6569, 1914, 7270, 3198, 7313, 511, 102, 2692, 1912, 7372, 1469, 1278, 4545, 7372, 924, 7397, 3309, 671, 5663, 711, 671, 2399, 8024, 1168, 3309, 1377, 5330, 924, 511, 102, 5445, 7028, 4565, 1469, 2195, 7372, 1377, 809, 6848, 2885, 8113, 2399, 510, 8114, 2399, 8024, 2772, 5442, 1372, 6206, 872, 2703, 2692, 8024, 1377, 809, 5388, 872, 671, 6777, 2094, 511, 102], [101, 924, 7397, 3309, 4507, 784, 720, 1104, 2137, 8043, 102, 924, 7397, 3309, 3221, 924, 7372, 1062, 1385, 2990, 897, 924, 7397, 4638, 2399, 7361, 8024, 738, 2218, 3221, 924, 7372, 1062, 1385, 833, 2190, 872, 6566, 6569, 1914, 7270, 3198, 7313, 511, 924, 7397, 3309, 6848, 2885, 4507, 2832, 924, 924, 7372, 772, 1501, 3198, 4638, 7564, 5050, 1104, 2137, 511, 102, 7028, 4565, 7372, 1469, 2195, 7372, 4638, 924, 7583, 3683, 924, 7397, 3198, 7270, 3291, 7028, 6206, 511, 7564, 5050, 679, 6639, 8024, 1377, 809, 6848, 2885, 924, 7397, 3309, 7361, 4764, 852, 924, 7583, 7770, 4638, 7028, 4565, 7372, 510, 2195, 7372, 8024, 5023, 1168, 1400, 3309, 7564, 5050, 1041, 6639, 1086, 711, 5632, 2346, 2904, 6848, 924, 7397, 3198, 7313, 3291, 7270, 4638, 7028, 4565, 7372, 2772, 2195, 7372, 511, 102], [101, 784, 720, 3221, 2160, 7361, 3309, 8043, 102, 2160, 7361, 3309, 3221, 924, 7372, 1062, 1385, 5314, 750, 2832, 924, 782, 2454, 6826, 5373, 6589, 4638, 2160, 7361, 3198, 7313, 511, 102], [101, 2160, 7361, 3309, 3300, 1914, 719, 8043, 102, 1914, 3144, 2160, 7361, 3309, 711, 8114, 172, 8183, 1921, 8024, 5632, 2418, 5373, 5287, 924, 6589, 722, 3189, 6629, 6369, 5050, 8039, 671, 2399, 3309, 772, 1501, 3766, 3300, 2160, 7361, 3309, 511, 102], [101, 2190, 2832, 924, 782, 4638, 2512, 1510, 8043, 102, 2160, 7361, 3309, 1079, 924, 7397, 679, 1359, 8038, 2160, 7361, 3309, 1079, 8024, 1315, 886, 3766, 3300, 769, 924, 6589, 8024, 1139, 7372, 898, 4197, 5543, 6608, 511, 102, 704, 3632, 3309, 1377, 809, 1908, 3126, 924, 1296, 8038, 6814, 749, 2160, 7361, 3309, 8024, 924, 1296, 3126, 1213, 704, 3632, 8024, 1139, 7372, 679, 6608, 8024, 697, 2399, 1079, 1377, 4509, 6435, 1908, 3126, 511, 102, 5303, 3632, 3309, 1400, 1372, 5543, 6842, 924, 8038, 924, 1296, 3612, 6589, 6631, 6814, 697, 2399, 8024, 1394, 1398, 5303, 3632, 8024, 1377, 6842, 924, 2897, 1726, 4385, 7032, 817, 966, 511, 102], [101, 784, 720, 3221, 5023, 2521, 3309, 8043, 102, 5023, 2521, 3309, 738, 1373, 6225, 2175, 3309, 1469, 1048, 6569, 3309, 8024, 5023, 2521, 3309, 1079, 1139, 4385, 924, 7372, 1062, 1385, 679, 2824, 2857, 6569, 818, 511, 102], [101, 5023, 2521, 3309, 3221, 1914, 719, 8043, 102, 7028, 4565, 7372, 1469, 2195, 7372, 8038, 671, 5663, 711, 8192, 172, 8420, 1921, 511, 102, 1278, 4545, 7372, 8038, 671, 5663, 711, 8114, 172, 8183, 1921, 511, 102, 135, 2692, 1912, 7372, 8038, 3187, 5023, 2521, 3309, 8024, 852, 7444, 6206, 3800, 2692, 924, 7372, 1394, 1398, 4495, 3126, 3189, 3309, 511, 102], [101, 2190, 2832, 924, 782, 4638, 2512, 1510, 8043, 102, 5023, 2521, 3309, 6632, 4764, 8024, 2190, 2832, 924, 782, 1469, 6158, 924, 7372, 782, 6632, 3300, 1164, 511, 102], [101, 784, 720, 3221, 4310, 6499, 3309, 8043, 102, 4310, 6499, 3309, 2900, 2832, 924, 782, 1762, 1358, 1168, 924, 7372, 1394, 1398, 1400, 8108, 1921, 8020, 7213, 6121, 924, 7372, 3940, 6887, 711, 8115, 1921, 8021, 1079, 8024, 1963, 679, 1398, 2692, 924, 7372, 1394, 1398, 1079, 2159, 8024, 1377, 2199, 1394, 1398, 6842, 2940, 924, 7372, 782, 2400, 4509, 6435, 3059, 7218, 511, 102, 1762, 3634, 3309, 7313, 8024, 924, 7372, 782, 1398, 2692, 2832, 924, 782, 4638, 4509, 6435, 8024, 3059, 7218, 1394, 1398, 2400, 6842, 6820, 2347, 3119, 1059, 6956, 924, 6589, 511, 102], [101, 784, 720, 3221, 3417, 924, 8043, 102, 1315, 924, 1296, 2144, 3417, 511, 2832, 924, 1184, 924, 7372, 1062, 1385, 4638, 2990, 7309, 8024, 1259, 2886, 6716, 860, 4307, 1105, 510, 5466, 689, 510, 3119, 1057, 5023, 8024, 6821, 702, 6814, 4923, 2218, 3221, 3417, 924, 511, 102], [101, 3417, 924, 833, 7309, 784, 720, 8043, 102, 978, 2434, 4307, 1105, 8038, 6716, 7770, 860, 7028, 510, 4567, 1380, 510, 2157, 3184, 4567, 1380, 5023, 511, 102, 1825, 3315, 2658, 1105, 8038, 2399, 7977, 510, 2595, 1166, 510, 5466, 689, 5023, 511, 102, 6568, 1218, 4307, 1105, 8038, 702, 782, 3119, 1057, 510, 6566, 965, 510, 924, 7583, 5023, 511, 102], [101, 3417, 924, 5310, 6389, 3300, 1525, 763, 8043, 102, 3633, 2382, 2824, 924, 510, 1217, 6589, 2824, 924, 510, 7370, 1912, 2824, 924, 510, 2454, 3309, 510, 2867, 924, 511, 102], [101, 6716, 860, 679, 1962, 2582, 720, 743, 924, 7372, 8043, 102, 3255, 5543, 3417, 924, 8038, 3378, 763, 772, 1501, 2990, 897, 8024, 1440, 6401, 924, 7372, 1062, 1385, 6716, 860, 2658, 1105, 1377, 809, 4989, 1315, 4761, 6887, 3221, 1415, 5543, 1916, 2832, 924, 511, 102, 782, 2339, 7564, 2144, 3417, 924, 8038, 1963, 3362, 679, 4802, 2137, 5632, 6716, 2658, 1105, 8024, 1377, 809, 5440, 5991, 782, 2339, 7564, 3417, 924, 511, 102, 978, 2434, 1440, 4761, 2160, 3351, 8038, 6848, 2885, 978, 2434, 1440, 4761, 3291, 2160, 3351, 4638, 772, 1501, 511, 102, 5296, 678, 1914, 2157, 2832, 924, 511, 102], [101, 784, 720, 3221, 4385, 7032, 817, 966, 8043, 102, 4385, 7032, 817, 966, 3221, 2900, 924, 7372, 1296, 4638, 6842, 924, 7032, 3144, 7583, 511, 1762, 924, 7372, 3309, 7361, 6772, 7270, 4638, 782, 2195, 924, 7372, 704, 8024, 4507, 754, 7023, 4500, 6642, 769, 924, 6589, 2772, 1772, 6130, 5283, 924, 6589, 1169, 2428, 8024, 924, 1296, 7555, 678, 4916, 5168, 3300, 671, 2137, 4638, 6569, 818, 1114, 1906, 7032, 8024, 6158, 924, 7372, 782, 6206, 3724, 6842, 924, 3198, 8024, 924, 7372, 782, 794, 6569, 818, 1114, 1906, 7032, 704, 2807, 7370, 671, 2137, 4638, 6842, 924, 2797, 5330, 6589, 8024, 865, 7583, 1315, 868, 711, 6842, 924, 7032, 113, 771, 4917, 1, 6237, 5276, 7032, 2, 114, 6842, 6820, 5314, 6158, 924, 7372, 782, 2772, 2832, 924, 782, 511, 102], [101, 784, 720, 3221, 1048, 6608, 7583, 8043, 102, 1048, 6608, 7583, 2900, 1762, 924, 7372, 1394, 1398, 704, 6226, 2137, 4638, 2938, 1927, 1762, 671, 2137, 7361, 2428, 1079, 924, 7372, 782, 1048, 6608, 985, 6569, 818, 4638, 7583, 2428, 511, 2900, 4507, 924, 7372, 782, 1469, 6158, 924, 7372, 782, 752, 1044, 5276, 2137, 8024, 2938, 1927, 7583, 1762, 6226, 2137, 3144, 7583, 722, 1079, 8024, 6158, 924, 7372, 782, 5632, 6121, 2824, 2857, 2938, 1927, 8024, 924, 7372, 782, 679, 6566, 6569, 6608, 985, 4638, 7583, 2428, 511, 671, 5663, 2100, 1762, 754, 1278, 4545, 7372, 680, 2692, 1912, 7372, 704, 511, 102], [101, 784, 720, 3221, 3297, 1920, 6411, 928, 1333, 1156, 8043, 102, 924, 7372, 1352, 3175, 1762, 5041, 6370, 1469, 2252, 6121, 924, 7372, 1394, 1398, 3198, 8024, 2553, 7557, 924, 2898, 3297, 1920, 4638, 6411, 2692, 8024, 757, 679, 3619, 7745, 1469, 7391, 4737, 8039, 2618, 2127, 1394, 1398, 4638, 2824, 6437, 8024, 1059, 7481, 2252, 6121, 5632, 2346, 2418, 2226, 4638, 721, 1218, 511, 1415, 1156, 8024, 2199, 2193, 5636, 924, 7372, 1394, 1398, 3187, 3126, 8024, 2772, 2824, 2857, 1071, 800, 3791, 2526, 1400, 3362, 511, 102], [101, 784, 720, 3221, 1440, 4761, 721, 1218, 8043, 102, 1440, 4761, 3221, 2832, 924, 782, 2772, 6158, 924, 7372, 782, 1762, 924, 7372, 1394, 1398, 5041, 6370, 1469, 2252, 6121, 4638, 6814, 4923, 704, 2190, 924, 7372, 3403, 4638, 3300, 1068, 752, 7555, 1403, 924, 7372, 782, 2792, 868, 4638, 4509, 2845, 1469, 7357, 6835, 511, 102, 143, 510, 6370, 4989, 1394, 1398, 3198, 8024, 3418, 2945, 924, 7372, 782, 4638, 6418, 7309, 8024, 2190, 2347, 4761, 2772, 2418, 4761, 4638, 7028, 6206, 752, 2141, 1963, 2141, 1440, 4761, 8039, 102, 144, 510, 1394, 1398, 6370, 4989, 1400, 8024, 1762, 1394, 1398, 3300, 3126, 3309, 1079, 8024, 2496, 1314, 7372, 2658, 1105, 1872, 1217, 3198, 8024, 2418, 1350, 3198, 1440, 4761, 924, 7372, 782, 8039, 102, 145, 510, 924, 7372, 3403, 4638, 1355, 4495, 6760, 4919, 2772, 924, 7372, 1394, 1398, 3300, 1068, 752, 7555, 1359, 1220, 3198, 8024, 2832, 924, 3175, 2418, 1350, 3198, 6858, 4761, 924, 7372, 782, 8039, 102, 146, 510, 924, 7372, 752, 3125, 1355, 4495, 1400, 8024, 2832, 924, 3175, 2418, 1350, 3198, 6858, 4761, 924, 7372, 782, 511, 102], [101, 784, 720, 3221, 6432, 3209, 721, 1218, 8043, 102, 924, 7372, 782, 4638, 6432, 3209, 721, 1218, 8024, 3221, 2900, 924, 7372, 782, 2418, 2496, 1403, 2832, 924, 782, 6432, 3209, 924, 7372, 1394, 1398, 3340, 3621, 4638, 1079, 2159, 8024, 4294, 1166, 3221, 1048, 6569, 3340, 3621, 1079, 2159, 4638, 721, 1218, 511, 924, 7372, 782, 6432, 3209, 4638, 1079, 2159, 8024, 712, 6206, 3221, 2512, 1510, 2832, 924, 782, 1104, 2137, 3221, 1415, 2832, 924, 1350, 1963, 862, 2832, 924, 4638, 671, 1147, 752, 7555, 511, 102], [101, 784, 720, 3221, 1908, 3126, 3340, 3621, 8043, 102, 2832, 924, 782, 1762, 924, 7372, 1394, 1398, 704, 3632, 1400, 4638, 671, 3667, 3198, 7313, 1079, 8024, 3300, 3326, 4509, 6435, 2612, 1908, 924, 1296, 3126, 1213, 8024, 1908, 3126, 3221, 2190, 1333, 1394, 1398, 3791, 2526, 3126, 1213, 4638, 2612, 1908, 8024, 679, 3121, 1359, 1333, 1394, 1398, 4638, 1392, 7555, 3326, 1164, 1469, 721, 1218, 511, 2769, 1744, 924, 7372, 3791, 6226, 2137, 4509, 6435, 1908, 3126, 4638, 3198, 7313, 711, 123, 2399, 8024, 6631, 6814, 749, 6821, 702, 3309, 7361, 8024, 2218, 679, 5543, 1908, 3126, 8024, 924, 1296, 5303, 3632, 511, 102], [101, 784, 720, 3221, 2399, 7977, 6428, 1440, 3340, 3621, 8043, 102, 1963, 3362, 2832, 924, 3198, 8024, 6428, 2845, 749, 6158, 924, 7372, 782, 4638, 2399, 7977, 8024, 924, 7372, 1394, 1398, 793, 4197, 3300, 3126, 8024, 852, 2418, 750, 809, 3291, 3633, 1469, 6444, 3146, 511, 2832, 924, 782, 4509, 2845, 4638, 6158, 924, 7372, 782, 2399, 7977, 679, 4696, 2141, 8024, 5636, 886, 2832, 924, 782, 3118, 802, 4638, 924, 7372, 6589, 2208, 754, 2418, 802, 924, 7372, 6589, 4638, 8024, 924, 7372, 782, 3300, 3326, 3291, 3633, 2400, 6206, 3724, 2832, 924, 782, 6133, 769, 924, 7372, 6589, 8024, 2772, 5442, 1762, 5314, 802, 924, 7372, 7032, 3198, 2902, 4212, 2141, 802, 924, 7372, 6589, 680, 2418, 802, 924, 7372, 6589, 4638, 3683, 891, 3118, 802, 511, 102], [101, 784, 720, 3221, 6225, 2175, 3309, 3340, 3621, 8043, 102, 788, 788, 898, 2945, 4567, 1325, 5023, 3300, 7361, 6598, 3160, 2523, 7410, 1161, 3171, 6158, 924, 7372, 782, 1762, 2832, 924, 3198, 3221, 1415, 2347, 5307, 2642, 3300, 3378, 4905, 4565, 4567, 8024, 711, 749, 7344, 3632, 2347, 3300, 4565, 4567, 4638, 782, 2372, 4567, 2832, 924, 8024, 924, 1296, 704, 6206, 6226, 2137, 671, 702, 6225, 2175, 3309, 511, 1762, 3634, 3309, 7313, 8024, 6158, 924, 7372, 782, 1728, 4565, 4567, 3118, 1139, 1278, 4545, 6589, 2772, 3119, 1057, 2938, 1927, 8024, 924, 7372, 782, 679, 6566, 6569, 8024, 1372, 3300, 6225, 2175, 3309, 4007, 722, 1400, 8024, 924, 1296, 2798, 3633, 2466, 4495, 3126, 511, 102], [101, 784, 720, 3221, 978, 2434, 7372, 2137, 721, 8043, 102, 2477, 6133, 6158, 924, 7372, 782, 1728, 4565, 4567, 2772, 2692, 1912, 839, 2154, 5445, 2970, 1358, 3780, 4545, 3198, 2792, 1355, 4495, 4638, 1278, 4545, 6589, 4500, 8024, 2772, 711, 6133, 985, 6158, 924, 7372, 782, 1728, 4565, 4567, 2772, 2692, 1912, 839, 2154, 2193, 5636, 839, 3655, 5445, 3187, 3791, 2339, 868, 3198, 4638, 3119, 1057, 2938, 1927, 809, 1350, 1728, 2399, 5439, 510, 4565, 4567, 1469, 2692, 1912, 2193, 5636, 7444, 7270, 3309, 2844, 4415, 4638, 2938, 1927, 4638, 671, 5102, 782, 6716, 924, 7372, 511, 102], [101, 784, 720, 3221, 1278, 4545, 7372, 8043, 102, 1315, 1278, 4545, 6589, 4500, 924, 7372, 8024, 2900, 809, 5276, 2137, 4638, 1278, 4545, 6589, 4500, 711, 5314, 802, 924, 7372, 7032, 3340, 816, 4638, 924, 7372, 8024, 1315, 2990, 897, 1278, 4545, 6589, 4500, 924, 7397, 4638, 924, 7372, 8024, 2124, 3221, 978, 2434, 924, 7372, 4638, 712, 6206, 1079, 2159, 722, 671, 8024, 679, 788, 1259, 2886, 1278, 4495, 4638, 1278, 4545, 6589, 1469, 2797, 3318, 6589, 4500, 8024, 6820, 1259, 2886, 857, 7368, 510, 2844, 4415, 510, 1278, 7368, 6392, 1906, 5023, 4638, 6589, 4500, 511, 102], [101, 1278, 4545, 7372, 1146, 5102, 102, 1278, 924, 8038, 1744, 2157, 5320, 5040, 4638, 1825, 3315, 1278, 4545, 924, 7372, 8024, 6208, 4667, 2408, 3793, 510, 924, 7397, 1825, 4794, 511, 102, 1555, 689, 1278, 924, 8038, 1555, 689, 924, 7372, 1062, 1385, 5307, 5852, 8024, 2902, 7444, 2832, 924, 510, 2130, 1587, 924, 7397, 511, 102], [101, 1278, 4545, 7372, 2582, 720, 6608, 8043, 102, 2845, 7218, 1798, 8038, 924, 7372, 1062, 1385, 2845, 7218, 2642, 5442, 1762, 1278, 7368, 7027, 2792, 5709, 6589, 4638, 1278, 4545, 6589, 8024, 671, 5663, 1146, 7305, 6402, 1278, 4545, 924, 7372, 680, 857, 7368, 1278, 4545, 924, 7372, 511, 102, 2137, 7583, 5314, 802, 1798, 8038, 2902, 1394, 1398, 5276, 2137, 2137, 7583, 5314, 802, 924, 7372, 7032, 511, 102], [101, 1278, 4545, 7372, 6844, 1394, 6443, 743, 8043, 102, 1744, 2157, 1278, 924, 2456, 6379, 782, 782, 1346, 924, 511, 102, 1555, 689, 1278, 924, 2456, 6379, 2682, 6206, 3291, 1059, 7481, 924, 7397, 4638, 8024, 2772, 2682, 2845, 7218, 5632, 6589, 5790, 510, 6822, 1366, 5790, 5023, 782, 5408, 6579, 743, 511, 2215, 1071, 3221, 5439, 782, 1469, 2111, 2094, 511, 102], [101, 6579, 743, 1278, 4545, 7372, 3198, 7444, 6206, 3800, 2692, 784, 720, 8043, 102, 1146, 3926, 924, 7372, 6569, 818, 5745, 1741, 8038, 857, 7368, 1278, 4545, 924, 7372, 679, 2824, 2857, 7305, 6402, 1278, 4545, 6589, 4500, 511, 102, 749, 6237, 924, 7372, 3309, 7361, 510, 4415, 6608, 3175, 2466, 8038, 1278, 4545, 7372, 671, 5663, 3221, 671, 2399, 4638, 8024, 7444, 6206, 3680, 2399, 5330, 924, 8024, 1278, 4545, 6589, 1044, 1807, 802, 1400, 4415, 6608, 511, 102, 3926, 3504, 1278, 4545, 7372, 4638, 1048, 6608, 7583, 510, 3683, 891, 5314, 802, 8020, 924, 7372, 1062, 1385, 2824, 2857, 1914, 2208, 3683, 891, 8021, 510, 5314, 802, 7361, 7583, 8020, 3297, 7770, 6608, 802, 1914, 2208, 8021, 511, 102], [101, 784, 720, 3221, 7028, 4565, 7372, 8043, 102, 7028, 1920, 4565, 4567, 7372, 8024, 3221, 2900, 4507, 924, 7372, 1062, 1385, 5307, 1215, 4638, 809, 4294, 2137, 7028, 1920, 4565, 4567, 8024, 1963, 2626, 2595, 5514, 4606, 510, 2552, 5491, 3453, 3647, 510, 5554, 3980, 6117, 5023, 711, 7599, 7372, 1355, 4495, 3198, 8024, 2496, 6158, 924, 782, 6809, 1168, 924, 7372, 3340, 3621, 2792, 5276, 2137, 4638, 7028, 1920, 4565, 4567, 4307, 2578, 1400, 8024, 4507, 924, 7372, 1062, 1385, 3418, 2945, 924, 7372, 1394, 1398, 5276, 2137, 3118, 802, 924, 7372, 7032, 4638, 1555, 689, 924, 7372, 6121, 711, 511, 102], [101, 7028, 4565, 7372, 2582, 720, 6608, 8043, 102, 2137, 7583, 5314, 802, 1798, 8038, 2902, 1394, 1398, 5276, 2137, 2137, 7583, 5314, 802, 924, 7372, 7032, 511, 102], [101, 7028, 4565, 7372, 6844, 1394, 6443, 743, 8043, 102, 7028, 4565, 7372, 2456, 6379, 868, 711, 1278, 4545, 7372, 4638, 6133, 1041, 924, 7372, 8024, 680, 1278, 4545, 7372, 671, 6629, 6579, 743, 8024, 924, 7397, 3291, 1217, 1059, 7481, 8024, 6844, 1394, 5439, 782, 1469, 2157, 2431, 7553, 3448, 3393, 511, 102], [101, 6579, 743, 7028, 4565, 7372, 3198, 7444, 6206, 3800, 2692, 784, 720, 8043, 102, 924, 7583, 6206, 1041, 6639, 8038, 743, 7028, 4565, 7372, 2218, 3221, 743, 924, 7583, 511, 7028, 4565, 7372, 4638, 868, 4500, 3221, 6208, 4667, 1920, 4567, 4638, 1278, 4545, 6589, 4500, 1469, 2434, 1908, 3309, 7313, 3187, 3791, 2339, 868, 4638, 3119, 1057, 2938, 1927, 8024, 924, 7583, 679, 743, 1916, 8024, 6929, 679, 1963, 679, 743, 511, 102, 3800, 2692, 6768, 4568, 924, 7397, 8038, 7028, 4565, 7372, 7370, 749, 6608, 802, 7028, 4565, 722, 1912, 8024, 2518, 2518, 6820, 1259, 1419, 6768, 4568, 511, 6768, 4568, 3221, 7478, 2382, 7028, 6206, 4638, 8024, 5318, 7478, 2769, 812, 6371, 711, 4638, 2207, 3688, 4567, 8024, 5445, 3221, 2900, 7028, 4565, 4638, 3193, 3309, 4307, 2578, 511, 3683, 1963, 4617, 4568, 4638, 6768, 4568, 4917, 711, 1333, 855, 4617, 8024, 1963, 3362, 679, 1350, 3193, 1355, 4385, 1469, 3780, 4545, 8024, 6768, 4568, 738, 833, 1359, 2768, 7028, 4565, 511, 102], [101, 784, 720, 3221, 857, 7368, 3823, 6585, 924, 7372, 8043, 102, 809, 857, 7368, 3189, 3144, 2902, 3189, 5314, 802, 857, 7368, 3823, 6585, 3418, 2945, 2797, 3318, 5023, 3780, 4545, 7555, 4680, 4638, 886, 4500, 3613, 5314, 802, 3780, 4545, 3823, 6585, 511, 102], [101, 784, 720, 3221, 4294, 3654, 4565, 4567, 1278, 4545, 7372, 8043, 102, 4294, 2137, 4565, 4567, 891, 1963, 7028, 1920, 4565, 4567, 924, 7372, 1469, 4617, 4568, 924, 7372, 8024, 924, 7372, 782, 2902, 5276, 2137, 4638, 7032, 7583, 5314, 802, 924, 7372, 7032, 511, 102], [101, 784, 720, 3221, 6568, 772, 924, 7372, 8043, 102, 924, 7372, 782, 2902, 924, 7372, 1394, 1398, 4638, 5276, 2137, 2190, 2792, 2824, 924, 4638, 6568, 772, 1350, 1071, 3300, 1068, 1164, 4660, 1728, 5632, 4197, 4135, 2154, 2772, 2692, 1912, 752, 3125, 6863, 2768, 4638, 2938, 1927, 2824, 2857, 6608, 985, 6569, 818, 4638, 924, 7372, 511, 102], [101, 784, 720, 3221, 821, 689, 6568, 772, 924, 7372, 8043, 102, 809, 821, 689, 2100, 3123, 1762, 1743, 2137, 1765, 4157, 4638, 6568, 772, 1469, 4289, 6598, 868, 711, 924, 7372, 3403, 4638, 4638, 6568, 772, 924, 7372, 511, 102], [101, 784, 720, 3221, 6817, 6783, 2339, 1072, 924, 7372, 8043, 102, 2824, 924, 1728, 6901, 1358, 5632, 4197, 4135, 2154, 1469, 2692, 1912, 752, 3125, 6863, 2768, 4638, 6817, 6783, 2339, 1072, 4638, 2938, 1927, 1350, 5018, 676, 5442, 2938, 2154, 6608, 985, 4638, 6569, 818, 4638, 924, 7372, 511, 102], [101, 784, 720, 3221, 6573, 4289, 6817, 6783, 924, 7372, 8043, 102, 809, 6817, 6783, 6854, 704, 4638, 6573, 4289, 868, 711, 924, 7372, 3403, 4638, 8024, 924, 7372, 782, 2190, 4507, 5632, 4197, 4135, 2154, 1469, 2692, 1912, 752, 3125, 6863, 2768, 4638, 6573, 4289, 2938, 1927, 6566, 6569, 6608, 985, 4638, 924, 7372, 511, 102], [101, 784, 720, 3221, 2339, 4923, 924, 7372, 8043, 102, 809, 2339, 4923, 7555, 4680, 704, 4638, 6568, 772, 2938, 1927, 1469, 4685, 1068, 3791, 2526, 6608, 985, 6569, 818, 711, 924, 7372, 3403, 4638, 4638, 924, 7372, 511, 102], [101, 784, 720, 3221, 6569, 818, 924, 7372, 8043, 102, 809, 6158, 924, 7372, 782, 2190, 5018, 676, 5442, 898, 3791, 2418, 6566, 4638, 6608, 985, 6569, 818, 711, 924, 7372, 3403, 4638, 4638, 924, 7372, 511, 102], [101, 784, 720, 3221, 1062, 830, 6569, 818, 924, 7372, 8043, 102, 2824, 924, 6158, 924, 7372, 782, 1762, 1062, 1066, 1767, 2792, 6822, 6121, 4495, 772, 510, 5307, 5852, 2772, 1071, 800, 3833, 1220, 3198, 8024, 1728, 1355, 4495, 2692, 1912, 752, 3125, 5445, 6863, 2768, 4638, 1062, 830, 782, 6716, 839, 767, 2772, 6568, 772, 2938, 1927, 8024, 898, 3791, 2418, 4507, 6158, 924, 7372, 782, 2824, 2857, 4638, 5307, 3845, 6608, 985, 6569, 818, 4638, 924, 7372, 511, 102], [101, 784, 720, 3221, 7416, 712, 6569, 818, 924, 7372, 8043, 102, 6158, 924, 7372, 782, 2792, 7416, 4500, 4638, 1447, 2339, 8024, 1762, 1358, 7416, 6814, 4923, 704, 794, 752, 924, 7372, 1296, 2792, 6770, 3209, 4638, 680, 6158, 924, 7372, 782, 4638, 689, 1218, 3300, 1068, 4638, 2339, 868, 3198, 8024, 1728, 6901, 1358, 2692, 1912, 752, 3125, 5445, 839, 767, 510, 3655, 2426, 2772, 1728, 2642, 3300, 680, 689, 1218, 3300, 1068, 4638, 5466, 689, 2595, 4565, 4567, 8024, 2792, 5636, 839, 3655, 2772, 3647, 767, 8024, 6158, 924, 7372, 782, 3418, 2945, 3791, 2526, 2772, 7416, 4500, 1394, 1398, 8024, 7557, 6566, 2857, 1278, 5790, 6589, 4500, 1350, 5307, 3845, 6608, 985, 6569, 818, 8024, 1259, 2886, 2418, 3118, 1139, 4638, 6401, 6390, 6589, 4500, 8024, 4507, 924, 7372, 782, 1762, 6226, 2137, 4638, 6608, 985, 7361, 7583, 1079, 6566, 6569, 6608, 985, 4638, 671, 4905, 924, 7372, 511, 102], [101, 784, 720, 3221, 772, 1501, 6569, 818, 924, 7372, 8043, 102, 6158, 924, 7372, 782, 2792, 4495, 772, 510, 1139, 1545, 4638, 772, 1501, 2772, 1555, 1501, 1762, 2824, 924, 1277, 1818, 1079, 1355, 4495, 752, 3125, 8024, 6863, 2768, 886, 4500, 510, 3867, 6589, 2772, 3082, 868, 6421, 772, 1501, 2772, 1555, 1501, 4638, 782, 2772, 1071, 800, 818, 862, 782, 4638, 782, 6716, 839, 2154, 510, 4565, 4567, 510, 3647, 767, 2772, 6568, 772, 2938, 1927, 8024, 898, 3791, 2418, 4507, 6158, 924, 7372, 782, 6566, 6569, 3198, 8024, 924, 7372, 782, 1762, 5276, 2137, 4638, 6608, 985, 7361, 7583, 1079, 6566, 6569, 6608, 985, 510, 2990, 897, 924, 7397, 4638, 671, 4905, 924, 7372, 511, 102], [101, 711, 784, 720, 6206, 711, 5632, 2346, 743, 924, 7372, 8043, 102, 2769, 812, 3719, 6823, 738, 679, 4761, 6887, 8024, 3209, 1921, 1469, 2692, 1912, 1525, 671, 702, 1044, 3341, 8024, 4495, 3833, 704, 1905, 1905, 2100, 1762, 4708, 2692, 1912, 510, 4565, 4567, 510, 6716, 3125, 4638, 7599, 7372, 102], [101, 711, 5632, 2346, 743, 924, 7372, 7444, 6206, 3800, 2692, 1525, 1126, 4157, 8043, 102, 924, 7372, 2832, 1057, 6206, 6844, 2428, 8024, 4415, 6568, 924, 7372, 3123, 3297, 1400, 102], [101, 711, 5632, 2346, 2832, 924, 7444, 6206, 6905, 2542, 784, 720, 7556, 2415, 8043, 102, 7151, 2190, 782, 6716, 7599, 7372, 1377, 809, 6981, 5390, 2692, 1912, 7372, 510, 1278, 4545, 7372, 510, 7028, 4565, 7372, 8024, 7151, 2190, 6568, 772, 7599, 7372, 1377, 809, 6981, 5390, 2195, 7372, 510, 2157, 6568, 7372, 5023, 511, 102], [101, 711, 5632, 2346, 2832, 924, 7444, 6206, 2582, 720, 6848, 8043, 102, 1278, 4545, 7372, 510, 2692, 1912, 7372, 510, 7028, 4565, 7372, 510, 2195, 7372, 704, 8024, 679, 1398, 4638, 772, 1501, 924, 6589, 510, 924, 7583, 510, 924, 7397, 6569, 818, 738, 833, 679, 1398, 102]]\nvalid 8 [[101, 784, 720, 3221, 5466, 689, 6569, 818, 924, 7372, 8043, 102, 809, 1392, 4905, 683, 689, 2825, 3318, 782, 1447, 1762, 794, 752, 5466, 689, 2825, 3318, 2339, 868, 3198, 1728, 4541, 2575, 2772, 6814, 1927, 6863, 2768, 1394, 1398, 2190, 3175, 2772, 800, 782, 4638, 782, 6716, 839, 2154, 2772, 6568, 772, 2938, 1927, 2792, 2193, 5636, 4638, 5307, 3845, 6608, 985, 6569, 818, 711, 2824, 924, 7599, 7372, 4638, 6569, 818, 924, 7372, 511, 102], [101, 784, 720, 3221, 1358, 4660, 782, 8043, 102, 2900, 2832, 924, 782, 1469, 6158, 924, 7372, 782, 2900, 2137, 775, 3300, 924, 7372, 7032, 6435, 3724, 3326, 4638, 782, 511, 102], [101, 1358, 4660, 782, 4638, 6206, 3724, 8043, 102, 2769, 1744, 3791, 2526, 2190, 1358, 4660, 782, 6598, 3419, 3187, 7361, 1169, 8024, 1358, 4660, 782, 1377, 809, 3221, 5632, 4197, 782, 1469, 3791, 782, 8024, 852, 2553, 7557, 4507, 2832, 924, 782, 2772, 6158, 924, 7372, 782, 2900, 2137, 511, 102], [101, 1358, 4660, 782, 4638, 3326, 1164, 8043, 102, 4509, 6435, 924, 7372, 4415, 6608, 8038, 1394, 1398, 5276, 2137, 4638, 924, 7372, 752, 3125, 1355, 4495, 1400, 8024, 1358, 4660, 782, 3300, 3326, 1403, 924, 7372, 782, 4509, 6435, 4415, 6608, 511, 102], [101, 784, 720, 3221, 924, 7372, 3309, 7361, 8043, 102, 1348, 4917, 924, 7372, 3309, 7313, 8024, 2900, 924, 7372, 1394, 1398, 3300, 3126, 3309, 511, 102], [101, 924, 7372, 3309, 7361, 2582, 720, 6226, 2137, 8043, 102, 924, 7372, 3309, 7361, 1377, 809, 2902, 2399, 3299, 510, 3189, 6369, 5050, 8024, 738, 1377, 809, 2902, 5661, 4923, 3309, 510, 2339, 4923, 3309, 510, 4495, 7270, 3309, 6369, 5050, 511, 2769, 1744, 671, 5663, 794, 5276, 2137, 3189, 4638, 7439, 4157, 2458, 1993, 8024, 5635, 1394, 1398, 3309, 4007, 3189, 8125, 4157, 511, 102], [101, 784, 720, 3221, 924, 7372, 7032, 7583, 8043, 102, 2218, 3221, 924, 7372, 1062, 1385, 6608, 802, 924, 7372, 7032, 4638, 3297, 7770, 7361, 7583, 511, 102], [101, 924, 7372, 7032, 7583, 2582, 720, 4802, 2137, 4638, 8043, 102, 6568, 772, 924, 7372, 8038, 2902, 4212, 924, 7372, 6568, 772, 4638, 2141, 7354, 817, 966, 510, 7028, 5390, 817, 3419, 5023, 3341, 4802, 2137, 511, 102, 782, 6716, 924, 7372, 8038, 3418, 2945, 2832, 924, 782, 7444, 6206, 1469, 5373, 6589, 5543, 1213, 3341, 4802, 2137, 511, 102]]\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n2022-12-20 03:05:38,331 - INFO - starting training\n2022-12-20 03:05:38,331 - INFO - starting training\n2022-12-20 03:05:38,331 - INFO - starting training\n2022-12-20 03:05:38,331 - INFO - starting training\n2022-12-20 03:05:38,331 - INFO - starting training\n2022-12-20 03:05:38,331 - INFO - starting training\n/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n2022-12-20 03:05:48,051 - INFO - batch 1 of epoch 1, loss 6.61671257019043, batch_acc 0.20781527531083482, lr [0.0]\n2022-12-20 03:05:48,051 - INFO - batch 1 of epoch 1, loss 6.61671257019043, batch_acc 0.20781527531083482, lr [0.0]\n2022-12-20 03:05:48,051 - INFO - batch 1 of epoch 1, loss 6.61671257019043, batch_acc 0.20781527531083482, lr [0.0]\n2022-12-20 03:05:48,051 - INFO - batch 1 of epoch 1, loss 6.61671257019043, batch_acc 0.20781527531083482, lr [0.0]\n2022-12-20 03:05:48,051 - INFO - batch 1 of epoch 1, loss 6.61671257019043, batch_acc 0.20781527531083482, lr [0.0]\n2022-12-20 03:05:48,051 - INFO - batch 1 of epoch 1, loss 6.61671257019043, batch_acc 0.20781527531083482, lr [0.0]\n2022-12-20 03:05:51,472 - INFO - batch 2 of epoch 1, loss 6.0683369636535645, batch_acc 0.21169354838709678, lr [0.0]\n2022-12-20 03:05:51,472 - INFO - batch 2 of epoch 1, loss 6.0683369636535645, batch_acc 0.21169354838709678, lr [0.0]\n2022-12-20 03:05:51,472 - INFO - batch 2 of epoch 1, loss 6.0683369636535645, batch_acc 0.21169354838709678, lr [0.0]\n2022-12-20 03:05:51,472 - INFO - batch 2 of epoch 1, loss 6.0683369636535645, batch_acc 0.21169354838709678, lr [0.0]\n2022-12-20 03:05:51,472 - INFO - batch 2 of epoch 1, loss 6.0683369636535645, batch_acc 0.21169354838709678, lr [0.0]\n2022-12-20 03:05:51,472 - INFO - batch 2 of epoch 1, loss 6.0683369636535645, batch_acc 0.21169354838709678, lr [0.0]\n2022-12-20 03:05:56,410 - INFO - batch 3 of epoch 1, loss 5.794827938079834, batch_acc 0.23450586264656617, lr [0.0]\n2022-12-20 03:05:56,410 - INFO - batch 3 of epoch 1, loss 5.794827938079834, batch_acc 0.23450586264656617, lr [0.0]\n2022-12-20 03:05:56,410 - INFO - batch 3 of epoch 1, loss 5.794827938079834, batch_acc 0.23450586264656617, lr [0.0]\n2022-12-20 03:05:56,410 - INFO - batch 3 of epoch 1, loss 5.794827938079834, batch_acc 0.23450586264656617, lr [0.0]\n2022-12-20 03:05:56,410 - INFO - batch 3 of epoch 1, loss 5.794827938079834, batch_acc 0.23450586264656617, lr [0.0]\n2022-12-20 03:05:56,410 - INFO - batch 3 of epoch 1, loss 5.794827938079834, batch_acc 0.23450586264656617, lr [0.0]\n2022-12-20 03:06:01,508 - INFO - batch 4 of epoch 1, loss 6.089877128601074, batch_acc 0.2236842105263158, lr [6.4999999999999995e-09]\n2022-12-20 03:06:01,508 - INFO - batch 4 of epoch 1, loss 6.089877128601074, batch_acc 0.2236842105263158, lr [6.4999999999999995e-09]\n2022-12-20 03:06:01,508 - INFO - batch 4 of epoch 1, loss 6.089877128601074, batch_acc 0.2236842105263158, lr [6.4999999999999995e-09]\n2022-12-20 03:06:01,508 - INFO - batch 4 of epoch 1, loss 6.089877128601074, batch_acc 0.2236842105263158, lr [6.4999999999999995e-09]\n2022-12-20 03:06:01,508 - INFO - batch 4 of epoch 1, loss 6.089877128601074, batch_acc 0.2236842105263158, lr [6.4999999999999995e-09]\n2022-12-20 03:06:01,508 - INFO - batch 4 of epoch 1, loss 6.089877128601074, batch_acc 0.2236842105263158, lr [6.4999999999999995e-09]\n2022-12-20 03:06:07,874 - INFO - batch 5 of epoch 1, loss 5.917581081390381, batch_acc 0.20569620253164558, lr [6.4999999999999995e-09]\n2022-12-20 03:06:07,874 - INFO - batch 5 of epoch 1, loss 5.917581081390381, batch_acc 0.20569620253164558, lr [6.4999999999999995e-09]\n2022-12-20 03:06:07,874 - INFO - batch 5 of epoch 1, loss 5.917581081390381, batch_acc 0.20569620253164558, lr [6.4999999999999995e-09]\n2022-12-20 03:06:07,874 - INFO - batch 5 of epoch 1, loss 5.917581081390381, batch_acc 0.20569620253164558, lr [6.4999999999999995e-09]\n2022-12-20 03:06:07,874 - INFO - batch 5 of epoch 1, loss 5.917581081390381, batch_acc 0.20569620253164558, lr [6.4999999999999995e-09]\n2022-12-20 03:06:07,874 - INFO - batch 5 of epoch 1, loss 5.917581081390381, batch_acc 0.20569620253164558, lr [6.4999999999999995e-09]\n2022-12-20 03:06:12,781 - INFO - batch 6 of epoch 1, loss 5.614741802215576, batch_acc 0.26752767527675275, lr [6.4999999999999995e-09]\n2022-12-20 03:06:12,781 - INFO - batch 6 of epoch 1, loss 5.614741802215576, batch_acc 0.26752767527675275, lr [6.4999999999999995e-09]\n2022-12-20 03:06:12,781 - INFO - batch 6 of epoch 1, loss 5.614741802215576, batch_acc 0.26752767527675275, lr [6.4999999999999995e-09]\n2022-12-20 03:06:12,781 - INFO - batch 6 of epoch 1, loss 5.614741802215576, batch_acc 0.26752767527675275, lr [6.4999999999999995e-09]\n2022-12-20 03:06:12,781 - INFO - batch 6 of epoch 1, loss 5.614741802215576, batch_acc 0.26752767527675275, lr [6.4999999999999995e-09]\n2022-12-20 03:06:12,781 - INFO - batch 6 of epoch 1, loss 5.614741802215576, batch_acc 0.26752767527675275, lr [6.4999999999999995e-09]\n2022-12-20 03:06:17,332 - INFO - batch 7 of epoch 1, loss 6.596742630004883, batch_acc 0.19210977701543738, lr [6.4999999999999995e-09]\n2022-12-20 03:06:17,332 - INFO - batch 7 of epoch 1, loss 6.596742630004883, batch_acc 0.19210977701543738, lr [6.4999999999999995e-09]\n2022-12-20 03:06:17,332 - INFO - batch 7 of epoch 1, loss 6.596742630004883, batch_acc 0.19210977701543738, lr [6.4999999999999995e-09]\n2022-12-20 03:06:17,332 - INFO - batch 7 of epoch 1, loss 6.596742630004883, batch_acc 0.19210977701543738, lr [6.4999999999999995e-09]\n2022-12-20 03:06:17,332 - INFO - batch 7 of epoch 1, loss 6.596742630004883, batch_acc 0.19210977701543738, lr [6.4999999999999995e-09]\n2022-12-20 03:06:17,332 - INFO - batch 7 of epoch 1, loss 6.596742630004883, batch_acc 0.19210977701543738, lr [6.4999999999999995e-09]\n2022-12-20 03:06:17,517 - INFO - epoch 1: loss 6.099831444876535, predict_acc 0.22016109348303636\n2022-12-20 03:06:17,517 - INFO - epoch 1: loss 6.099831444876535, predict_acc 0.22016109348303636\n2022-12-20 03:06:17,517 - INFO - epoch 1: loss 6.099831444876535, predict_acc 0.22016109348303636\n2022-12-20 03:06:17,517 - INFO - epoch 1: loss 6.099831444876535, predict_acc 0.22016109348303636\n2022-12-20 03:06:17,517 - INFO - epoch 1: loss 6.099831444876535, predict_acc 0.22016109348303636\n2022-12-20 03:06:17,517 - INFO - epoch 1: loss 6.099831444876535, predict_acc 0.22016109348303636\n2022-12-20 03:06:17,523 - INFO - saving model for epoch 1\n2022-12-20 03:06:17,523 - INFO - saving model for epoch 1\n2022-12-20 03:06:17,523 - INFO - saving model for epoch 1\n2022-12-20 03:06:17,523 - INFO - saving model for epoch 1\n2022-12-20 03:06:17,523 - INFO - saving model for epoch 1\n2022-12-20 03:06:17,523 - INFO - saving model for epoch 1\n2022-12-20 03:06:18,012 - INFO - epoch 1 finished\n2022-12-20 03:06:18,012 - INFO - epoch 1 finished\n2022-12-20 03:06:18,012 - INFO - epoch 1 finished\n2022-12-20 03:06:18,012 - INFO - epoch 1 finished\n2022-12-20 03:06:18,012 - INFO - epoch 1 finished\n2022-12-20 03:06:18,012 - INFO - epoch 1 finished\n2022-12-20 03:06:18,017 - INFO - time for one epoch: 0:00:39.679736\n2022-12-20 03:06:18,017 - INFO - time for one epoch: 0:00:39.679736\n2022-12-20 03:06:18,017 - INFO - time for one epoch: 0:00:39.679736\n2022-12-20 03:06:18,017 - INFO - time for one epoch: 0:00:39.679736\n2022-12-20 03:06:18,017 - INFO - time for one epoch: 0:00:39.679736\n2022-12-20 03:06:18,017 - INFO - time for one epoch: 0:00:39.679736\n2022-12-20 03:06:18,053 - INFO - start validating 1\n2022-12-20 03:06:18,053 - INFO - start validating 1\n2022-12-20 03:06:18,053 - INFO - start validating 1\n2022-12-20 03:06:18,053 - INFO - start validating 1\n2022-12-20 03:06:18,053 - INFO - start validating 1\n2022-12-20 03:06:18,053 - INFO - start validating 1\n2022-12-20 03:06:19,948 - INFO - validate epoch 1: loss 5.58495569229126\n2022-12-20 03:06:19,948 - INFO - validate epoch 1: loss 5.58495569229126\n2022-12-20 03:06:19,948 - INFO - validate epoch 1: loss 5.58495569229126\n2022-12-20 03:06:19,948 - INFO - validate epoch 1: loss 5.58495569229126\n2022-12-20 03:06:19,948 - INFO - validate epoch 1: loss 5.58495569229126\n2022-12-20 03:06:19,948 - INFO - validate epoch 1: loss 5.58495569229126\n2022-12-20 03:06:19,959 - INFO - time for validating one epoch: 0:00:01.895937\n2022-12-20 03:06:19,959 - INFO - time for validating one epoch: 0:00:01.895937\n2022-12-20 03:06:19,959 - INFO - time for validating one epoch: 0:00:01.895937\n2022-12-20 03:06:19,959 - INFO - time for validating one epoch: 0:00:01.895937\n2022-12-20 03:06:19,959 - INFO - time for validating one epoch: 0:00:01.895937\n2022-12-20 03:06:19,959 - INFO - time for validating one epoch: 0:00:01.895937\n2022-12-20 03:06:19,970 - INFO - saving current best model for epoch 1\n2022-12-20 03:06:19,970 - INFO - saving current best model for epoch 1\n2022-12-20 03:06:19,970 - INFO - saving current best model for epoch 1\n2022-12-20 03:06:19,970 - INFO - saving current best model for epoch 1\n2022-12-20 03:06:19,970 - INFO - saving current best model for epoch 1\n2022-12-20 03:06:19,970 - INFO - saving current best model for epoch 1\n2022-12-20 03:06:29,545 - INFO - batch 1 of epoch 2, loss 6.1293110847473145, batch_acc 0.2293103448275862, lr [6.4999999999999995e-09]\n2022-12-20 03:06:29,545 - INFO - batch 1 of epoch 2, loss 6.1293110847473145, batch_acc 0.2293103448275862, lr [6.4999999999999995e-09]\n2022-12-20 03:06:29,545 - INFO - batch 1 of epoch 2, loss 6.1293110847473145, batch_acc 0.2293103448275862, lr [6.4999999999999995e-09]\n2022-12-20 03:06:29,545 - INFO - batch 1 of epoch 2, loss 6.1293110847473145, batch_acc 0.2293103448275862, lr [6.4999999999999995e-09]\n2022-12-20 03:06:29,545 - INFO - batch 1 of epoch 2, loss 6.1293110847473145, batch_acc 0.2293103448275862, lr [6.4999999999999995e-09]\n2022-12-20 03:06:29,545 - INFO - batch 1 of epoch 2, loss 6.1293110847473145, batch_acc 0.2293103448275862, lr [6.4999999999999995e-09]\n2022-12-20 03:06:35,769 - INFO - batch 2 of epoch 2, loss 5.859013080596924, batch_acc 0.21285140562248997, lr [6.4999999999999995e-09]\n2022-12-20 03:06:35,769 - INFO - batch 2 of epoch 2, loss 5.859013080596924, batch_acc 0.21285140562248997, lr [6.4999999999999995e-09]\n2022-12-20 03:06:35,769 - INFO - batch 2 of epoch 2, loss 5.859013080596924, batch_acc 0.21285140562248997, lr [6.4999999999999995e-09]\n2022-12-20 03:06:35,769 - INFO - batch 2 of epoch 2, loss 5.859013080596924, batch_acc 0.21285140562248997, lr [6.4999999999999995e-09]\n2022-12-20 03:06:35,769 - INFO - batch 2 of epoch 2, loss 5.859013080596924, batch_acc 0.21285140562248997, lr [6.4999999999999995e-09]\n2022-12-20 03:06:35,769 - INFO - batch 2 of epoch 2, loss 5.859013080596924, batch_acc 0.21285140562248997, lr [6.4999999999999995e-09]\n2022-12-20 03:06:40,130 - INFO - batch 3 of epoch 2, loss 5.965485095977783, batch_acc 0.21663778162911612, lr [6.4999999999999995e-09]\n2022-12-20 03:06:40,130 - INFO - batch 3 of epoch 2, loss 5.965485095977783, batch_acc 0.21663778162911612, lr [6.4999999999999995e-09]\n2022-12-20 03:06:40,130 - INFO - batch 3 of epoch 2, loss 5.965485095977783, batch_acc 0.21663778162911612, lr [6.4999999999999995e-09]\n2022-12-20 03:06:40,130 - INFO - batch 3 of epoch 2, loss 5.965485095977783, batch_acc 0.21663778162911612, lr [6.4999999999999995e-09]\n2022-12-20 03:06:40,130 - INFO - batch 3 of epoch 2, loss 5.965485095977783, batch_acc 0.21663778162911612, lr [6.4999999999999995e-09]\n2022-12-20 03:06:40,130 - INFO - batch 3 of epoch 2, loss 5.965485095977783, batch_acc 0.21663778162911612, lr [6.4999999999999995e-09]\n2022-12-20 03:06:44,146 - INFO - batch 4 of epoch 2, loss 6.688068389892578, batch_acc 0.2017291066282421, lr [1.2999999999999999e-08]\n2022-12-20 03:06:44,146 - INFO - batch 4 of epoch 2, loss 6.688068389892578, batch_acc 0.2017291066282421, lr [1.2999999999999999e-08]\n2022-12-20 03:06:44,146 - INFO - batch 4 of epoch 2, loss 6.688068389892578, batch_acc 0.2017291066282421, lr [1.2999999999999999e-08]\n2022-12-20 03:06:44,146 - INFO - batch 4 of epoch 2, loss 6.688068389892578, batch_acc 0.2017291066282421, lr [1.2999999999999999e-08]\n2022-12-20 03:06:44,146 - INFO - batch 4 of epoch 2, loss 6.688068389892578, batch_acc 0.2017291066282421, lr [1.2999999999999999e-08]\n2022-12-20 03:06:44,146 - INFO - batch 4 of epoch 2, loss 6.688068389892578, batch_acc 0.2017291066282421, lr [1.2999999999999999e-08]\n2022-12-20 03:06:49,823 - INFO - batch 5 of epoch 2, loss 6.221829414367676, batch_acc 0.19538188277087035, lr [1.2999999999999999e-08]\n2022-12-20 03:06:49,823 - INFO - batch 5 of epoch 2, loss 6.221829414367676, batch_acc 0.19538188277087035, lr [1.2999999999999999e-08]\n2022-12-20 03:06:49,823 - INFO - batch 5 of epoch 2, loss 6.221829414367676, batch_acc 0.19538188277087035, lr [1.2999999999999999e-08]\n2022-12-20 03:06:49,823 - INFO - batch 5 of epoch 2, loss 6.221829414367676, batch_acc 0.19538188277087035, lr [1.2999999999999999e-08]\n2022-12-20 03:06:49,823 - INFO - batch 5 of epoch 2, loss 6.221829414367676, batch_acc 0.19538188277087035, lr [1.2999999999999999e-08]\n2022-12-20 03:06:49,823 - INFO - batch 5 of epoch 2, loss 6.221829414367676, batch_acc 0.19538188277087035, lr [1.2999999999999999e-08]\n2022-12-20 03:06:53,752 - INFO - batch 6 of epoch 2, loss 6.435229778289795, batch_acc 0.22402597402597402, lr [1.2999999999999999e-08]\n2022-12-20 03:06:53,752 - INFO - batch 6 of epoch 2, loss 6.435229778289795, batch_acc 0.22402597402597402, lr [1.2999999999999999e-08]\n2022-12-20 03:06:53,752 - INFO - batch 6 of epoch 2, loss 6.435229778289795, batch_acc 0.22402597402597402, lr [1.2999999999999999e-08]\n2022-12-20 03:06:53,752 - INFO - batch 6 of epoch 2, loss 6.435229778289795, batch_acc 0.22402597402597402, lr [1.2999999999999999e-08]\n2022-12-20 03:06:53,752 - INFO - batch 6 of epoch 2, loss 6.435229778289795, batch_acc 0.22402597402597402, lr [1.2999999999999999e-08]\n2022-12-20 03:06:53,752 - INFO - batch 6 of epoch 2, loss 6.435229778289795, batch_acc 0.22402597402597402, lr [1.2999999999999999e-08]\n2022-12-20 03:06:58,559 - INFO - batch 7 of epoch 2, loss 5.807709217071533, batch_acc 0.22661396574440051, lr [1.2999999999999999e-08]\n2022-12-20 03:06:58,559 - INFO - batch 7 of epoch 2, loss 5.807709217071533, batch_acc 0.22661396574440051, lr [1.2999999999999999e-08]\n2022-12-20 03:06:58,559 - INFO - batch 7 of epoch 2, loss 5.807709217071533, batch_acc 0.22661396574440051, lr [1.2999999999999999e-08]\n2022-12-20 03:06:58,559 - INFO - batch 7 of epoch 2, loss 5.807709217071533, batch_acc 0.22661396574440051, lr [1.2999999999999999e-08]\n2022-12-20 03:06:58,559 - INFO - batch 7 of epoch 2, loss 5.807709217071533, batch_acc 0.22661396574440051, lr [1.2999999999999999e-08]\n2022-12-20 03:06:58,559 - INFO - batch 7 of epoch 2, loss 5.807709217071533, batch_acc 0.22661396574440051, lr [1.2999999999999999e-08]\n2022-12-20 03:06:58,849 - INFO - epoch 2: loss 6.1580922944205145, predict_acc 0.21651945571735498\n2022-12-20 03:06:58,849 - INFO - epoch 2: loss 6.1580922944205145, predict_acc 0.21651945571735498\n2022-12-20 03:06:58,849 - INFO - epoch 2: loss 6.1580922944205145, predict_acc 0.21651945571735498\n2022-12-20 03:06:58,849 - INFO - epoch 2: loss 6.1580922944205145, predict_acc 0.21651945571735498\n2022-12-20 03:06:58,849 - INFO - epoch 2: loss 6.1580922944205145, predict_acc 0.21651945571735498\n2022-12-20 03:06:58,849 - INFO - epoch 2: loss 6.1580922944205145, predict_acc 0.21651945571735498\n2022-12-20 03:06:58,858 - INFO - saving model for epoch 2\n2022-12-20 03:06:58,858 - INFO - saving model for epoch 2\n2022-12-20 03:06:58,858 - INFO - saving model for epoch 2\n2022-12-20 03:06:58,858 - INFO - saving model for epoch 2\n2022-12-20 03:06:58,858 - INFO - saving model for epoch 2\n2022-12-20 03:06:58,858 - INFO - saving model for epoch 2\n2022-12-20 03:06:59,327 - INFO - epoch 2 finished\n2022-12-20 03:06:59,327 - INFO - epoch 2 finished\n2022-12-20 03:06:59,327 - INFO - epoch 2 finished\n2022-12-20 03:06:59,327 - INFO - epoch 2 finished\n2022-12-20 03:06:59,327 - INFO - epoch 2 finished\n2022-12-20 03:06:59,327 - INFO - epoch 2 finished\n2022-12-20 03:06:59,334 - INFO - time for one epoch: 0:00:38.886134\n2022-12-20 03:06:59,334 - INFO - time for one epoch: 0:00:38.886134\n2022-12-20 03:06:59,334 - INFO - time for one epoch: 0:00:38.886134\n2022-12-20 03:06:59,334 - INFO - time for one epoch: 0:00:38.886134\n2022-12-20 03:06:59,334 - INFO - time for one epoch: 0:00:38.886134\n2022-12-20 03:06:59,334 - INFO - time for one epoch: 0:00:38.886134\n2022-12-20 03:06:59,372 - INFO - start validating 1\n2022-12-20 03:06:59,372 - INFO - start validating 1\n2022-12-20 03:06:59,372 - INFO - start validating 1\n2022-12-20 03:06:59,372 - INFO - start validating 1\n2022-12-20 03:06:59,372 - INFO - start validating 1\n2022-12-20 03:06:59,372 - INFO - start validating 1\n2022-12-20 03:07:01,180 - INFO - validate epoch 2: loss 5.584933280944824\n2022-12-20 03:07:01,180 - INFO - validate epoch 2: loss 5.584933280944824\n2022-12-20 03:07:01,180 - INFO - validate epoch 2: loss 5.584933280944824\n2022-12-20 03:07:01,180 - INFO - validate epoch 2: loss 5.584933280944824\n2022-12-20 03:07:01,180 - INFO - validate epoch 2: loss 5.584933280944824\n2022-12-20 03:07:01,180 - INFO - validate epoch 2: loss 5.584933280944824\n2022-12-20 03:07:01,191 - INFO - time for validating one epoch: 0:00:01.811561\n2022-12-20 03:07:01,191 - INFO - time for validating one epoch: 0:00:01.811561\n2022-12-20 03:07:01,191 - INFO - time for validating one epoch: 0:00:01.811561\n2022-12-20 03:07:01,191 - INFO - time for validating one epoch: 0:00:01.811561\n2022-12-20 03:07:01,191 - INFO - time for validating one epoch: 0:00:01.811561\n2022-12-20 03:07:01,191 - INFO - time for validating one epoch: 0:00:01.811561\n2022-12-20 03:07:01,201 - INFO - saving current best model for epoch 2\n2022-12-20 03:07:01,201 - INFO - saving current best model for epoch 2\n2022-12-20 03:07:01,201 - INFO - saving current best model for epoch 2\n2022-12-20 03:07:01,201 - INFO - saving current best model for epoch 2\n2022-12-20 03:07:01,201 - INFO - saving current best model for epoch 2\n2022-12-20 03:07:01,201 - INFO - saving current best model for epoch 2\n2022-12-20 03:07:11,035 - INFO - batch 1 of epoch 3, loss 6.647725582122803, batch_acc 0.19887429643527205, lr [1.2999999999999999e-08]\n2022-12-20 03:07:11,035 - INFO - batch 1 of epoch 3, loss 6.647725582122803, batch_acc 0.19887429643527205, lr [1.2999999999999999e-08]\n2022-12-20 03:07:11,035 - INFO - batch 1 of epoch 3, loss 6.647725582122803, batch_acc 0.19887429643527205, lr [1.2999999999999999e-08]\n2022-12-20 03:07:11,035 - INFO - batch 1 of epoch 3, loss 6.647725582122803, batch_acc 0.19887429643527205, lr [1.2999999999999999e-08]\n2022-12-20 03:07:11,035 - INFO - batch 1 of epoch 3, loss 6.647725582122803, batch_acc 0.19887429643527205, lr [1.2999999999999999e-08]\n2022-12-20 03:07:11,035 - INFO - batch 1 of epoch 3, loss 6.647725582122803, batch_acc 0.19887429643527205, lr [1.2999999999999999e-08]\n2022-12-20 03:07:13,849 - INFO - batch 2 of epoch 3, loss 6.054856300354004, batch_acc 0.21271393643031786, lr [1.2999999999999999e-08]\n2022-12-20 03:07:13,849 - INFO - batch 2 of epoch 3, loss 6.054856300354004, batch_acc 0.21271393643031786, lr [1.2999999999999999e-08]\n2022-12-20 03:07:13,849 - INFO - batch 2 of epoch 3, loss 6.054856300354004, batch_acc 0.21271393643031786, lr [1.2999999999999999e-08]\n2022-12-20 03:07:13,849 - INFO - batch 2 of epoch 3, loss 6.054856300354004, batch_acc 0.21271393643031786, lr [1.2999999999999999e-08]\n2022-12-20 03:07:13,849 - INFO - batch 2 of epoch 3, loss 6.054856300354004, batch_acc 0.21271393643031786, lr [1.2999999999999999e-08]\n2022-12-20 03:07:13,849 - INFO - batch 2 of epoch 3, loss 6.054856300354004, batch_acc 0.21271393643031786, lr [1.2999999999999999e-08]\n2022-12-20 03:07:19,851 - INFO - batch 3 of epoch 3, loss 6.086453437805176, batch_acc 0.23655913978494625, lr [1.2999999999999999e-08]\n2022-12-20 03:07:19,851 - INFO - batch 3 of epoch 3, loss 6.086453437805176, batch_acc 0.23655913978494625, lr [1.2999999999999999e-08]\n2022-12-20 03:07:19,851 - INFO - batch 3 of epoch 3, loss 6.086453437805176, batch_acc 0.23655913978494625, lr [1.2999999999999999e-08]\n2022-12-20 03:07:19,851 - INFO - batch 3 of epoch 3, loss 6.086453437805176, batch_acc 0.23655913978494625, lr [1.2999999999999999e-08]\n2022-12-20 03:07:19,851 - INFO - batch 3 of epoch 3, loss 6.086453437805176, batch_acc 0.23655913978494625, lr [1.2999999999999999e-08]\n2022-12-20 03:07:19,851 - INFO - batch 3 of epoch 3, loss 6.086453437805176, batch_acc 0.23655913978494625, lr [1.2999999999999999e-08]\n2022-12-20 03:07:25,840 - INFO - batch 4 of epoch 3, loss 5.931734085083008, batch_acc 0.23247863247863249, lr [1.95e-08]\n2022-12-20 03:07:25,840 - INFO - batch 4 of epoch 3, loss 5.931734085083008, batch_acc 0.23247863247863249, lr [1.95e-08]\n2022-12-20 03:07:25,840 - INFO - batch 4 of epoch 3, loss 5.931734085083008, batch_acc 0.23247863247863249, lr [1.95e-08]\n2022-12-20 03:07:25,840 - INFO - batch 4 of epoch 3, loss 5.931734085083008, batch_acc 0.23247863247863249, lr [1.95e-08]\n2022-12-20 03:07:25,840 - INFO - batch 4 of epoch 3, loss 5.931734085083008, batch_acc 0.23247863247863249, lr [1.95e-08]\n2022-12-20 03:07:25,840 - INFO - batch 4 of epoch 3, loss 5.931734085083008, batch_acc 0.23247863247863249, lr [1.95e-08]\n2022-12-20 03:07:30,929 - INFO - batch 5 of epoch 3, loss 6.0269694328308105, batch_acc 0.22897196261682243, lr [1.95e-08]\n2022-12-20 03:07:30,929 - INFO - batch 5 of epoch 3, loss 6.0269694328308105, batch_acc 0.22897196261682243, lr [1.95e-08]\n2022-12-20 03:07:30,929 - INFO - batch 5 of epoch 3, loss 6.0269694328308105, batch_acc 0.22897196261682243, lr [1.95e-08]\n2022-12-20 03:07:30,929 - INFO - batch 5 of epoch 3, loss 6.0269694328308105, batch_acc 0.22897196261682243, lr [1.95e-08]\n2022-12-20 03:07:30,929 - INFO - batch 5 of epoch 3, loss 6.0269694328308105, batch_acc 0.22897196261682243, lr [1.95e-08]\n2022-12-20 03:07:30,929 - INFO - batch 5 of epoch 3, loss 6.0269694328308105, batch_acc 0.22897196261682243, lr [1.95e-08]\n2022-12-20 03:07:36,875 - INFO - batch 6 of epoch 3, loss 5.938654899597168, batch_acc 0.22672064777327935, lr [1.95e-08]\n2022-12-20 03:07:36,875 - INFO - batch 6 of epoch 3, loss 5.938654899597168, batch_acc 0.22672064777327935, lr [1.95e-08]\n2022-12-20 03:07:36,875 - INFO - batch 6 of epoch 3, loss 5.938654899597168, batch_acc 0.22672064777327935, lr [1.95e-08]\n2022-12-20 03:07:36,875 - INFO - batch 6 of epoch 3, loss 5.938654899597168, batch_acc 0.22672064777327935, lr [1.95e-08]\n2022-12-20 03:07:36,875 - INFO - batch 6 of epoch 3, loss 5.938654899597168, batch_acc 0.22672064777327935, lr [1.95e-08]\n2022-12-20 03:07:36,875 - INFO - batch 6 of epoch 3, loss 5.938654899597168, batch_acc 0.22672064777327935, lr [1.95e-08]\n2022-12-20 03:07:41,662 - INFO - batch 7 of epoch 3, loss 6.165643215179443, batch_acc 0.20592592592592593, lr [1.95e-08]\n2022-12-20 03:07:41,662 - INFO - batch 7 of epoch 3, loss 6.165643215179443, batch_acc 0.20592592592592593, lr [1.95e-08]\n2022-12-20 03:07:41,662 - INFO - batch 7 of epoch 3, loss 6.165643215179443, batch_acc 0.20592592592592593, lr [1.95e-08]\n2022-12-20 03:07:41,662 - INFO - batch 7 of epoch 3, loss 6.165643215179443, batch_acc 0.20592592592592593, lr [1.95e-08]\n2022-12-20 03:07:41,662 - INFO - batch 7 of epoch 3, loss 6.165643215179443, batch_acc 0.20592592592592593, lr [1.95e-08]\n2022-12-20 03:07:41,662 - INFO - batch 7 of epoch 3, loss 6.165643215179443, batch_acc 0.20592592592592593, lr [1.95e-08]\n2022-12-20 03:07:41,959 - INFO - epoch 3: loss 6.121719564710345, predict_acc 0.22085445329471398\n2022-12-20 03:07:41,959 - INFO - epoch 3: loss 6.121719564710345, predict_acc 0.22085445329471398\n2022-12-20 03:07:41,959 - INFO - epoch 3: loss 6.121719564710345, predict_acc 0.22085445329471398\n2022-12-20 03:07:41,959 - INFO - epoch 3: loss 6.121719564710345, predict_acc 0.22085445329471398\n2022-12-20 03:07:41,959 - INFO - epoch 3: loss 6.121719564710345, predict_acc 0.22085445329471398\n2022-12-20 03:07:41,959 - INFO - epoch 3: loss 6.121719564710345, predict_acc 0.22085445329471398\n2022-12-20 03:07:41,968 - INFO - saving model for epoch 3\n2022-12-20 03:07:41,968 - INFO - saving model for epoch 3\n2022-12-20 03:07:41,968 - INFO - saving model for epoch 3\n2022-12-20 03:07:41,968 - INFO - saving model for epoch 3\n2022-12-20 03:07:41,968 - INFO - saving model for epoch 3\n2022-12-20 03:07:41,968 - INFO - saving model for epoch 3\n2022-12-20 03:07:42,442 - INFO - epoch 3 finished\n2022-12-20 03:07:42,442 - INFO - epoch 3 finished\n2022-12-20 03:07:42,442 - INFO - epoch 3 finished\n2022-12-20 03:07:42,442 - INFO - epoch 3 finished\n2022-12-20 03:07:42,442 - INFO - epoch 3 finished\n2022-12-20 03:07:42,442 - INFO - epoch 3 finished\n2022-12-20 03:07:42,448 - INFO - time for one epoch: 0:00:40.521781\n2022-12-20 03:07:42,448 - INFO - time for one epoch: 0:00:40.521781\n2022-12-20 03:07:42,448 - INFO - time for one epoch: 0:00:40.521781\n2022-12-20 03:07:42,448 - INFO - time for one epoch: 0:00:40.521781\n2022-12-20 03:07:42,448 - INFO - time for one epoch: 0:00:40.521781\n2022-12-20 03:07:42,448 - INFO - time for one epoch: 0:00:40.521781\n2022-12-20 03:07:42,473 - INFO - start validating 1\n2022-12-20 03:07:42,473 - INFO - start validating 1\n2022-12-20 03:07:42,473 - INFO - start validating 1\n2022-12-20 03:07:42,473 - INFO - start validating 1\n2022-12-20 03:07:42,473 - INFO - start validating 1\n2022-12-20 03:07:42,473 - INFO - start validating 1\n2022-12-20 03:07:44,281 - INFO - validate epoch 3: loss 5.584875583648682\n2022-12-20 03:07:44,281 - INFO - validate epoch 3: loss 5.584875583648682\n2022-12-20 03:07:44,281 - INFO - validate epoch 3: loss 5.584875583648682\n2022-12-20 03:07:44,281 - INFO - validate epoch 3: loss 5.584875583648682\n2022-12-20 03:07:44,281 - INFO - validate epoch 3: loss 5.584875583648682\n2022-12-20 03:07:44,281 - INFO - validate epoch 3: loss 5.584875583648682\n2022-12-20 03:07:44,292 - INFO - time for validating one epoch: 0:00:01.810989\n2022-12-20 03:07:44,292 - INFO - time for validating one epoch: 0:00:01.810989\n2022-12-20 03:07:44,292 - INFO - time for validating one epoch: 0:00:01.810989\n2022-12-20 03:07:44,292 - INFO - time for validating one epoch: 0:00:01.810989\n2022-12-20 03:07:44,292 - INFO - time for validating one epoch: 0:00:01.810989\n2022-12-20 03:07:44,292 - INFO - time for validating one epoch: 0:00:01.810989\n2022-12-20 03:07:44,300 - INFO - saving current best model for epoch 3\n2022-12-20 03:07:44,300 - INFO - saving current best model for epoch 3\n2022-12-20 03:07:44,300 - INFO - saving current best model for epoch 3\n2022-12-20 03:07:44,300 - INFO - saving current best model for epoch 3\n2022-12-20 03:07:44,300 - INFO - saving current best model for epoch 3\n2022-12-20 03:07:44,300 - INFO - saving current best model for epoch 3\n","output_type":"stream"}]}]}